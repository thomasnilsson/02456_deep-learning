{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "4.3-EXE-CIFAR-10.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "bu1Wy6Xb81Sn"
      },
      "source": [
        "# Credits\n",
        "\n",
        "This is heavily influenced from https://github.com/pytorch/tutorials"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "oZW0gaQO81Sq"
      },
      "source": [
        "# CIFAR-10\n",
        "\n",
        "In thins notebook you need to put what you have learned into practice, and create your own convolutional classifier for the CIFAR-10 dataset.\n",
        "\n",
        "It has the classes: ‘airplane’, ‘automobile’, ‘bird’, ‘cat’, ‘deer’, ‘dog’, ‘frog’, ‘horse’, ‘ship’, ‘truck’.\n",
        "The images in CIFAR-10 are of size 3x32x32, i.e. 3-channel color images of 32x32 pixels in size.\n",
        "\n",
        "![cifar10](https://github.com/DeepLearningDTU/02456-deep-learning-with-PyTorch/blob/master/static_files/cifar10.png?raw=1)\n",
        "\n",
        "\n",
        "In order to train a classifier the following steps needs to be performed:\n",
        "\n",
        "1. Load and normalizing the CIFAR10 training and test datasets using\n",
        "   ``torchvision``\n",
        "2. Define a Convolutional Neural Network\n",
        "3. Define a loss function\n",
        "4. Train the network on the training data\n",
        "5. Test the network on the test data\n",
        "\n",
        "We will help you along the way.\n",
        "We indicate the places you need to modify the code with `# Your code here!`.\n",
        "It is however a good idea to read the entire assignment before you begin coding!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "htyg7xxN81St"
      },
      "source": [
        "## 1. Loading and normalizing CIFAR10\n",
        "\n",
        "Using ``torchvision``, it’s extremely easy to load CIFAR10."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "v3u2GIWr81Su",
        "colab": {}
      },
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "xx5SHRkm81S0"
      },
      "source": [
        "The output of torchvision datasets are PILImage images of range [0, 1].\n",
        "We transform them to Tensors of normalized range [-1, 1]\n",
        "\n",
        "**NB** Modify the code below to only use a small part of the dataset if your computer is very slow."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "QZeTujLC81S3",
        "outputId": "02bcfe47-b986-4b62-9c6e-010b91216188",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5, 0.5, 0.5),\n",
        "                          (0.5, 0.5, 0.5))]\n",
        ")\n",
        "\n",
        "# Load dataset\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=transform)\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                       download=True, transform=transform)\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat', 'deer',\n",
        "           'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "\n",
        "used_categories = range(len(classes))\n",
        "\n",
        "## USE CODE BELOW IF YOUR COMPUTER IS TOO SLOW\n",
        "reduce_dataset = False\n",
        "if reduce_dataset:\n",
        "    used_categories = (3, 5) # cats and dogs\n",
        "\n",
        "    classes = [classes[i] for i in used_categories]\n",
        "    new_train_data = []\n",
        "    new_train_labels = []\n",
        "\n",
        "    new_test_data = []\n",
        "    new_test_labels = []\n",
        "    for i, t in enumerate(used_categories):\n",
        "        new_train_data.append(trainset.data[np.where(np.array(trainset.targets) == t)])\n",
        "        new_train_labels += [i for _ in range(new_train_data[-1].shape[0])]\n",
        "\n",
        "        new_test_data.append(testset.data[np.where(np.array(testset.targets) == t)])\n",
        "        new_test_labels += [i for _ in range(new_test_data[-1].shape[0])]\n",
        "\n",
        "    new_train_data = np.concatenate(new_train_data, 0)\n",
        "    trainset.train_data = new_train_data\n",
        "    trainset.train_labels = new_train_labels\n",
        "\n",
        "    new_test_data = np.concatenate(new_test_data, 0)\n",
        "    testset.test_data = new_test_data\n",
        "    testset.test_labels = new_test_labels\n",
        "\n",
        "    \n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=4,\n",
        "                                          shuffle=True, num_workers=2)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=4,\n",
        "                                         shuffle=True, num_workers=2)\n",
        "train_data_iter = iter(trainloader)\n",
        "test_data_iter = iter(testloader)\n",
        "print('used classes:', classes)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "used classes: ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "JDHkc52L81S9",
        "outputId": "6ebb7af3-2781-4f9c-b55c-2b856156a26d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "print(\"Training data\")\n",
        "print(trainset.data.shape)\n",
        "print(len(trainset.targets))\n",
        "print()\n",
        "\n",
        "print(\"Test data\")\n",
        "print(testset.data.shape)\n",
        "print(len(testset.targets))\n",
        "print()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training data\n",
            "(50000, 32, 32, 3)\n",
            "50000\n",
            "\n",
            "Test data\n",
            "(10000, 32, 32, 3)\n",
            "10000\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "xSA1h94681TB"
      },
      "source": [
        "Let us show some of the training images, for fun.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "njJy0klP81TD",
        "outputId": "c4635766-7dd9-45a8-d85a-d2c593211d1d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        }
      },
      "source": [
        "# Run this cell multiple time to see more samples\n",
        "classes = ('plane', 'car', 'bird', 'cat', 'deer',\n",
        "           'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "\n",
        "def imshow(img):\n",
        "    \"\"\" show an image \"\"\"\n",
        "    img = img / 2 + 0.5 # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "\n",
        "\n",
        "# get some random training images\n",
        "images, labels = train_data_iter.next()\n",
        "\n",
        "# show images\n",
        "imshow(torchvision.utils.make_grid(images))\n",
        "\n",
        "# print labels\n",
        "print(' '.join('%5s' % classes[labels[j]] for j in range(4)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " bird  bird   car  deer\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAB6CAYAAACvHqiXAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/wHmWgMO35NCqwAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Wt3BVFMF81TI"
      },
      "source": [
        "## 2. Define a Convolutional Neural Network\n",
        "\n",
        "**Assignment 1:** Define a convolutional neural network. \n",
        "You may use the code from previous notebooks.\n",
        "We suggest that you start with a small network, and make sure that everything is working.\n",
        "Once you can train successfully come back and improve the architecture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_EsKbw3o81TK",
        "outputId": "71f35fa9-7ec5-47ff-968e-ee6c0fd90cc9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 544
        }
      },
      "source": [
        "from torch.autograd import Variable\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(Net, self).__init__()\n",
        "        self.num_classes = num_classes\n",
        "        \n",
        "        # A simple conv. nn\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=3,\n",
        "                      out_channels=8, \n",
        "                      kernel_size=7, \n",
        "                      padding=3),\n",
        "            nn.MaxPool2d(kernel_size=2, \n",
        "                         stride=2),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(in_channels=8, \n",
        "                      out_channels=16, \n",
        "                      kernel_size=5, \n",
        "                      padding=2),\n",
        "            nn.MaxPool2d(kernel_size=2,\n",
        "                         stride=2),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "        \n",
        "        # Fully connected part of network\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(in_features=16 * 8 * 8, #new input dimension original height/(2*2) \n",
        "                      out_features=32,\n",
        "                      bias=True),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(in_features=32, \n",
        "                      out_features=self.num_classes,\n",
        "                      bias=True))\n",
        "\n",
        "    def forward(self, x):\n",
        "        # First we pass the output through the convolutional layers\n",
        "        x = self.conv(x)\n",
        "        # We then flatten\n",
        "        x = x.view(-1, 16 * 8 * 8)\n",
        "        # lastly we return the output of the feedforward part after it has been undergone the softmax\n",
        "        return F.log_softmax(self.fc(x), dim=1) # Note we use softmax since we want a classification using labels\n",
        "    \n",
        "\n",
        "net = Net(len(used_categories))\n",
        "print(net)\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print('##converting network to cuda-enabled')\n",
        "    net.cuda()\n",
        "print(net)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Net(\n",
            "  (conv): Sequential(\n",
            "    (0): Conv2d(3, 8, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
            "    (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (2): ReLU(inplace)\n",
            "    (3): Conv2d(8, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
            "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (5): ReLU(inplace)\n",
            "  )\n",
            "  (fc): Sequential(\n",
            "    (0): Linear(in_features=1024, out_features=32, bias=True)\n",
            "    (1): ReLU(inplace)\n",
            "    (2): Linear(in_features=32, out_features=10, bias=True)\n",
            "  )\n",
            ")\n",
            "##converting network to cuda-enabled\n",
            "Net(\n",
            "  (conv): Sequential(\n",
            "    (0): Conv2d(3, 8, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
            "    (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (2): ReLU(inplace)\n",
            "    (3): Conv2d(8, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
            "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (5): ReLU(inplace)\n",
            "  )\n",
            "  (fc): Sequential(\n",
            "    (0): Linear(in_features=1024, out_features=32, bias=True)\n",
            "    (1): ReLU(inplace)\n",
            "    (2): Linear(in_features=32, out_features=10, bias=True)\n",
            "  )\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "7-IUg3sq81TQ"
      },
      "source": [
        "## 3. Define a Loss function and optimizer\n",
        "\n",
        "**Assignment 2:** Implement the criterion and optimizer. \n",
        "We suggest Classification Cross-Entropy loss and SGD with momentum.\n",
        "You might need to experiment a bit with the learning rate."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "48AX85QP81TR",
        "colab": {}
      },
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(net.parameters(), lr=0.001)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "-WneIN7C81TV"
      },
      "source": [
        "## 4. Train the network\n",
        "\n",
        "**Assignment 3:** Finish the training loop below. \n",
        "Start by using a small number of epochs (e.g. 3).\n",
        "Even with a low number of epochs you should be able to see results that are better than chance.\n",
        "When everything is working increase the number of epochs to find out how good your network really is."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "NkUanRRb81TW",
        "outputId": "a2e4821f-412b-4875-f81a-4add45446620",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "num_epoch = 20\n",
        "\n",
        "for epoch in range(num_epoch):  # loop over the dataset multiple times\n",
        "\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        # get the inputs\n",
        "        inputs, labels = data\n",
        "        # wrap them in Variable\n",
        "        inputs, labels = Variable(inputs).cuda(), Variable(labels).cuda()\n",
        "\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "        # forward + backward + optimize\n",
        "        output = net(inputs)\n",
        "        loss = criterion(output, labels)\n",
        "        \n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # print statistics\n",
        "        running_loss += loss.item()\n",
        "        if i % 1000 == 999:    # print every 1000 mini-batches\n",
        "            print('[%d, %5d] loss: %.3f' %\n",
        "                  (epoch + 1, i + 1, running_loss / 1000))\n",
        "            running_loss = 0.0\n",
        "\n",
        "print('Finished Training')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1,  1000] loss: 1.996\n",
            "[1,  2000] loss: 1.691\n",
            "[1,  3000] loss: 1.649\n",
            "[1,  4000] loss: 1.557\n",
            "[1,  5000] loss: 1.509\n",
            "[1,  6000] loss: 1.484\n",
            "[1,  7000] loss: 1.444\n",
            "[1,  8000] loss: 1.445\n",
            "[1,  9000] loss: 1.415\n",
            "[1, 10000] loss: 1.375\n",
            "[1, 11000] loss: 1.380\n",
            "[1, 12000] loss: 1.353\n",
            "[2,  1000] loss: 1.281\n",
            "[2,  2000] loss: 1.278\n",
            "[2,  3000] loss: 1.285\n",
            "[2,  4000] loss: 1.279\n",
            "[2,  5000] loss: 1.272\n",
            "[2,  6000] loss: 1.288\n",
            "[2,  7000] loss: 1.277\n",
            "[2,  8000] loss: 1.234\n",
            "[2,  9000] loss: 1.249\n",
            "[2, 10000] loss: 1.262\n",
            "[2, 11000] loss: 1.232\n",
            "[2, 12000] loss: 1.235\n",
            "[3,  1000] loss: 1.197\n",
            "[3,  2000] loss: 1.196\n",
            "[3,  3000] loss: 1.161\n",
            "[3,  4000] loss: 1.172\n",
            "[3,  5000] loss: 1.174\n",
            "[3,  6000] loss: 1.168\n",
            "[3,  7000] loss: 1.145\n",
            "[3,  8000] loss: 1.144\n",
            "[3,  9000] loss: 1.160\n",
            "[3, 10000] loss: 1.157\n",
            "[3, 11000] loss: 1.166\n",
            "[3, 12000] loss: 1.143\n",
            "[4,  1000] loss: 1.086\n",
            "[4,  2000] loss: 1.081\n",
            "[4,  3000] loss: 1.094\n",
            "[4,  4000] loss: 1.080\n",
            "[4,  5000] loss: 1.099\n",
            "[4,  6000] loss: 1.107\n",
            "[4,  7000] loss: 1.117\n",
            "[4,  8000] loss: 1.084\n",
            "[4,  9000] loss: 1.079\n",
            "[4, 10000] loss: 1.119\n",
            "[4, 11000] loss: 1.117\n",
            "[4, 12000] loss: 1.106\n",
            "[5,  1000] loss: 1.036\n",
            "[5,  2000] loss: 1.038\n",
            "[5,  3000] loss: 1.038\n",
            "[5,  4000] loss: 1.057\n",
            "[5,  5000] loss: 1.091\n",
            "[5,  6000] loss: 1.058\n",
            "[5,  7000] loss: 1.086\n",
            "[5,  8000] loss: 1.055\n",
            "[5,  9000] loss: 1.038\n",
            "[5, 10000] loss: 1.071\n",
            "[5, 11000] loss: 1.069\n",
            "[5, 12000] loss: 1.055\n",
            "[6,  1000] loss: 1.006\n",
            "[6,  2000] loss: 1.005\n",
            "[6,  3000] loss: 1.004\n",
            "[6,  4000] loss: 1.014\n",
            "[6,  5000] loss: 1.004\n",
            "[6,  6000] loss: 1.045\n",
            "[6,  7000] loss: 1.020\n",
            "[6,  8000] loss: 1.022\n",
            "[6,  9000] loss: 1.018\n",
            "[6, 10000] loss: 0.999\n",
            "[6, 11000] loss: 1.010\n",
            "[6, 12000] loss: 1.037\n",
            "[7,  1000] loss: 0.962\n",
            "[7,  2000] loss: 0.971\n",
            "[7,  3000] loss: 0.957\n",
            "[7,  4000] loss: 0.995\n",
            "[7,  5000] loss: 1.004\n",
            "[7,  6000] loss: 0.966\n",
            "[7,  7000] loss: 0.963\n",
            "[7,  8000] loss: 0.995\n",
            "[7,  9000] loss: 0.988\n",
            "[7, 10000] loss: 1.018\n",
            "[7, 11000] loss: 0.978\n",
            "[7, 12000] loss: 0.994\n",
            "[8,  1000] loss: 0.922\n",
            "[8,  2000] loss: 0.914\n",
            "[8,  3000] loss: 0.941\n",
            "[8,  4000] loss: 0.959\n",
            "[8,  5000] loss: 0.950\n",
            "[8,  6000] loss: 0.990\n",
            "[8,  7000] loss: 0.965\n",
            "[8,  8000] loss: 0.971\n",
            "[8,  9000] loss: 0.966\n",
            "[8, 10000] loss: 0.973\n",
            "[8, 11000] loss: 0.953\n",
            "[8, 12000] loss: 0.978\n",
            "[9,  1000] loss: 0.895\n",
            "[9,  2000] loss: 0.942\n",
            "[9,  3000] loss: 0.924\n",
            "[9,  4000] loss: 0.920\n",
            "[9,  5000] loss: 0.960\n",
            "[9,  6000] loss: 0.942\n",
            "[9,  7000] loss: 0.930\n",
            "[9,  8000] loss: 0.951\n",
            "[9,  9000] loss: 0.956\n",
            "[9, 10000] loss: 0.967\n",
            "[9, 11000] loss: 0.954\n",
            "[9, 12000] loss: 0.949\n",
            "[10,  1000] loss: 0.887\n",
            "[10,  2000] loss: 0.895\n",
            "[10,  3000] loss: 0.900\n",
            "[10,  4000] loss: 0.915\n",
            "[10,  5000] loss: 0.947\n",
            "[10,  6000] loss: 0.943\n",
            "[10,  7000] loss: 0.942\n",
            "[10,  8000] loss: 0.933\n",
            "[10,  9000] loss: 0.935\n",
            "[10, 10000] loss: 0.908\n",
            "[10, 11000] loss: 0.959\n",
            "[10, 12000] loss: 0.922\n",
            "[11,  1000] loss: 0.885\n",
            "[11,  2000] loss: 0.872\n",
            "[11,  3000] loss: 0.916\n",
            "[11,  4000] loss: 0.906\n",
            "[11,  5000] loss: 0.894\n",
            "[11,  6000] loss: 0.906\n",
            "[11,  7000] loss: 0.908\n",
            "[11,  8000] loss: 0.932\n",
            "[11,  9000] loss: 0.941\n",
            "[11, 10000] loss: 0.894\n",
            "[11, 11000] loss: 0.923\n",
            "[11, 12000] loss: 0.929\n",
            "[12,  1000] loss: 0.856\n",
            "[12,  2000] loss: 0.850\n",
            "[12,  3000] loss: 0.854\n",
            "[12,  4000] loss: 0.905\n",
            "[12,  5000] loss: 0.868\n",
            "[12,  6000] loss: 0.914\n",
            "[12,  7000] loss: 0.890\n",
            "[12,  8000] loss: 0.929\n",
            "[12,  9000] loss: 0.893\n",
            "[12, 10000] loss: 0.944\n",
            "[12, 11000] loss: 0.926\n",
            "[12, 12000] loss: 0.894\n",
            "[13,  1000] loss: 0.856\n",
            "[13,  2000] loss: 0.862\n",
            "[13,  3000] loss: 0.869\n",
            "[13,  4000] loss: 0.884\n",
            "[13,  5000] loss: 0.870\n",
            "[13,  6000] loss: 0.874\n",
            "[13,  7000] loss: 0.891\n",
            "[13,  8000] loss: 0.908\n",
            "[13,  9000] loss: 0.914\n",
            "[13, 10000] loss: 0.891\n",
            "[13, 11000] loss: 0.891\n",
            "[13, 12000] loss: 0.910\n",
            "[14,  1000] loss: 0.803\n",
            "[14,  2000] loss: 0.862\n",
            "[14,  3000] loss: 0.843\n",
            "[14,  4000] loss: 0.880\n",
            "[14,  5000] loss: 0.844\n",
            "[14,  6000] loss: 0.860\n",
            "[14,  7000] loss: 0.845\n",
            "[14,  8000] loss: 0.900\n",
            "[14,  9000] loss: 0.914\n",
            "[14, 10000] loss: 0.884\n",
            "[14, 11000] loss: 0.909\n",
            "[14, 12000] loss: 0.939\n",
            "[15,  1000] loss: 0.830\n",
            "[15,  2000] loss: 0.843\n",
            "[15,  3000] loss: 0.840\n",
            "[15,  4000] loss: 0.832\n",
            "[15,  5000] loss: 0.864\n",
            "[15,  6000] loss: 0.873\n",
            "[15,  7000] loss: 0.873\n",
            "[15,  8000] loss: 0.883\n",
            "[15,  9000] loss: 0.875\n",
            "[15, 10000] loss: 0.886\n",
            "[15, 11000] loss: 0.894\n",
            "[15, 12000] loss: 0.873\n",
            "[16,  1000] loss: 0.818\n",
            "[16,  2000] loss: 0.838\n",
            "[16,  3000] loss: 0.824\n",
            "[16,  4000] loss: 0.843\n",
            "[16,  5000] loss: 0.867\n",
            "[16,  6000] loss: 0.877\n",
            "[16,  7000] loss: 0.869\n",
            "[16,  8000] loss: 0.851\n",
            "[16,  9000] loss: 0.851\n",
            "[16, 10000] loss: 0.897\n",
            "[16, 11000] loss: 0.883\n",
            "[16, 12000] loss: 0.855\n",
            "[17,  1000] loss: 0.804\n",
            "[17,  2000] loss: 0.820\n",
            "[17,  3000] loss: 0.853\n",
            "[17,  4000] loss: 0.848\n",
            "[17,  5000] loss: 0.864\n",
            "[17,  6000] loss: 0.857\n",
            "[17,  7000] loss: 0.834\n",
            "[17,  8000] loss: 0.876\n",
            "[17,  9000] loss: 0.846\n",
            "[17, 10000] loss: 0.867\n",
            "[17, 11000] loss: 0.905\n",
            "[17, 12000] loss: 0.866\n",
            "[18,  1000] loss: 0.780\n",
            "[18,  2000] loss: 0.817\n",
            "[18,  3000] loss: 0.864\n",
            "[18,  4000] loss: 0.842\n",
            "[18,  5000] loss: 0.832\n",
            "[18,  6000] loss: 0.863\n",
            "[18,  7000] loss: 0.856\n",
            "[18,  8000] loss: 0.853\n",
            "[18,  9000] loss: 0.834\n",
            "[18, 10000] loss: 0.855\n",
            "[18, 11000] loss: 0.838\n",
            "[18, 12000] loss: 0.858\n",
            "[19,  1000] loss: 0.810\n",
            "[19,  2000] loss: 0.810\n",
            "[19,  3000] loss: 0.833\n",
            "[19,  4000] loss: 0.786\n",
            "[19,  5000] loss: 0.840\n",
            "[19,  6000] loss: 0.848\n",
            "[19,  7000] loss: 0.857\n",
            "[19,  8000] loss: 0.845\n",
            "[19,  9000] loss: 0.842\n",
            "[19, 10000] loss: 0.878\n",
            "[19, 11000] loss: 0.835\n",
            "[19, 12000] loss: 0.853\n",
            "[20,  1000] loss: 0.772\n",
            "[20,  2000] loss: 0.800\n",
            "[20,  3000] loss: 0.822\n",
            "[20,  4000] loss: 0.828\n",
            "[20,  5000] loss: 0.840\n",
            "[20,  6000] loss: 0.847\n",
            "[20,  7000] loss: 0.831\n",
            "[20,  8000] loss: 0.858\n",
            "[20,  9000] loss: 0.862\n",
            "[20, 10000] loss: 0.848\n",
            "[20, 11000] loss: 0.827\n",
            "[20, 12000] loss: 0.837\n",
            "Finished Training\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "0qAsbC8I81Ta"
      },
      "source": [
        "## 5. Test the network on the test data\n",
        "\n",
        "Now we need to check if the network has learnt anything at all.\n",
        "We will check this by predicting the class label that the neural network outputs, and checking it against the ground truth.\n",
        "If the prediction is correct, we add the sample to the list of correct predictions.\n",
        "\n",
        "Okay, first step. Let us display an image from the test set to get familiar."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7LT0RoAC81Tc",
        "outputId": "10aeb1eb-5676-4d30-ca40-9f0fb9fbf281",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "source": [
        "images, labels = test_data_iter.next()\n",
        "\n",
        "# print images\n",
        "imshow(torchvision.utils.make_grid(images))\n",
        "plt.show()\n",
        "\n",
        "print('GroundTruth:  ', ' '.join('%5s' % classes[labels[j]] for j in range(4)))\n",
        "\n",
        "_, predicted = torch.max(outputs.data, 1)\n",
        "print('Predicted:    ', ' '.join('%5s' % classes[predicted[j]] for j in range(4)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAB6CAYAAACvHqiXAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "GroundTruth:     car  deer plane  deer\n",
            "Predicted:      ship plane   car   dog\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ISA6LJJO81Tg"
      },
      "source": [
        "Let us look at how the network performs on the whole dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Smv6_BwF81Ti",
        "outputId": "af156246-74fe-4701-cd4f-9bbc79cdbfdd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "for data in testloader:\n",
        "    images, labels = data\n",
        "    outputs = net(Variable(images).cuda())\n",
        "    _, predicted = torch.max(outputs.data, 1)\n",
        "    total += labels.size(0)\n",
        "    correct += (predicted == labels.cuda()).sum()\n",
        "\n",
        "print('Accuracy of the network on the {} test images: {:4.2f} %'.format(\n",
        "    testset.data.shape[0], 100 * correct / total))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of the network on the 10000 test images: 64.00 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "QMZRvhaW81Tl"
      },
      "source": [
        "Hopefully the network is better than chance, which is $\\frac{1}{\\text{number of classes}}$ accuracy (randomly picking\n",
        "a class).\n",
        "\n",
        "\n",
        "We can also examine which class the network found the most difficult (makes more sense if you have many clases):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "WqVTQgKq81Tl",
        "outputId": "f9496697-89e3-4bad-b6e5-02b5de0e93a6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "class_total = list(0. for i in range(len(classes)))\n",
        "class_correct = list(0. for i in range(len(classes)))\n",
        "\n",
        "for data in testloader:\n",
        "    images, labels = data\n",
        "    outputs = net(Variable(images).cuda())\n",
        "    _, predicted = torch.max(outputs.data, 1)\n",
        "    c = (predicted == labels.cuda()).squeeze()\n",
        "    \n",
        "    for i in range(len(c)):\n",
        "        label = labels[i]\n",
        "        class_correct[label] += c[i].cpu().numpy()\n",
        "        class_total[label] += 1\n",
        "\n",
        "for i in range(len(classes)):\n",
        "    print('Accuracy of {:5s} : {:5.2f} %'.format(\n",
        "        classes[i], 100 * class_correct[i] / class_total[i]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of plane : 62.80 %\n",
            "Accuracy of car   : 71.90 %\n",
            "Accuracy of bird  : 52.00 %\n",
            "Accuracy of cat   : 52.50 %\n",
            "Accuracy of deer  : 57.00 %\n",
            "Accuracy of dog   : 49.90 %\n",
            "Accuracy of frog  : 71.70 %\n",
            "Accuracy of horse : 68.20 %\n",
            "Accuracy of ship  : 77.60 %\n",
            "Accuracy of truck : 80.70 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ocnQOBAl81Tn"
      },
      "source": [
        "**Assignment 4:** \n",
        "1. Go back and improve performance of the network. \n",
        " * If you are using all 10 classes you should get a test accuracy above 55%, but see how much further you can get it!\n",
        " * If you are using only 2 classes (e.g. cat and dog) you should get a test accuracy above 60%, but see how much further you can get it!\n",
        "\n",
        "2. Briefly describe what you did and any experiments you did along the way as well as what results you obtained.\n",
        "Did anything surprise you during the exercise?\n",
        "\n",
        "3. Write down key lessons/insights you got (if any) during this exercise.\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "I am going to try out transfer learning since this is one of the obvious cases where there is much to gain, note we allow the training of our transferred model which could be omitted so that only the last layer(s) are trained."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FsSn9cKFq2wZ",
        "colab_type": "code",
        "outputId": "4a3d0d15-c408-4bbe-80f3-a3db0cfc7d4f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from torchvision import models\n",
        "model = models.resnet18(pretrained=True)\n",
        "#for param in model.parameters():\n",
        "#    param.requires_grad = False\n",
        "\n",
        "# Parameters of newly constructed modules have requires_grad=True by default\n",
        "num_ftrs = model.fc.in_features\n",
        "model.fc = nn.Sequential(nn.Linear(num_ftrs, len(classes)), nn.LogSoftmax(dim=1))\n",
        "\n",
        "criteration = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "model.cuda()\n",
        "print(model)\n",
        "\n",
        "num_epoch = 20\n",
        "\n",
        "for epoch in range(num_epoch):  # loop over the dataset multiple times\n",
        "\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        # get the inputs\n",
        "        inputs, labels = data\n",
        "        # wrap them in Variable\n",
        "        inputs, labels = Variable(inputs).cuda(), Variable(labels).cuda()\n",
        "\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "        # forward + backward + optimize\n",
        "        output = model(inputs)\n",
        "        loss = criterion(output, labels)\n",
        "        \n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # print statistics\n",
        "        running_loss += loss.item()\n",
        "        if i % 1000 == 999:    # print every 1000 mini-batches\n",
        "            print('[%d, %5d] loss: %.3f' %\n",
        "                  (epoch + 1, i + 1, running_loss / 1000))\n",
        "            running_loss = 0.0\n",
        "\n",
        "print('Finished Training')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ResNet(\n",
            "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
            "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (relu): ReLU(inplace)\n",
            "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "  (layer1): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (layer2): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (layer3): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (layer4): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "  (fc): Sequential(\n",
            "    (0): Linear(in_features=512, out_features=10, bias=True)\n",
            "    (1): LogSoftmax()\n",
            "  )\n",
            ")\n",
            "[1,  1000] loss: 2.482\n",
            "[1,  2000] loss: 2.412\n",
            "[1,  3000] loss: 2.348\n",
            "[1,  4000] loss: 2.266\n",
            "[1,  5000] loss: 2.275\n",
            "[1,  6000] loss: 2.300\n",
            "[1,  7000] loss: 2.258\n",
            "[1,  8000] loss: 2.168\n",
            "[1,  9000] loss: 2.115\n",
            "[1, 10000] loss: 2.067\n",
            "[1, 11000] loss: 2.084\n",
            "[1, 12000] loss: 2.010\n",
            "[2,  1000] loss: 1.942\n",
            "[2,  2000] loss: 1.974\n",
            "[2,  3000] loss: 1.942\n",
            "[2,  4000] loss: 1.905\n",
            "[2,  5000] loss: 1.868\n",
            "[2,  6000] loss: 1.854\n",
            "[2,  7000] loss: 1.828\n",
            "[2,  8000] loss: 1.769\n",
            "[2,  9000] loss: 1.900\n",
            "[2, 10000] loss: 1.853\n",
            "[2, 11000] loss: 1.824\n",
            "[2, 12000] loss: 1.741\n",
            "[3,  1000] loss: 1.737\n",
            "[3,  2000] loss: 1.698\n",
            "[3,  3000] loss: 1.659\n",
            "[3,  4000] loss: 1.642\n",
            "[3,  5000] loss: 1.628\n",
            "[3,  6000] loss: 1.592\n",
            "[3,  7000] loss: 1.715\n",
            "[3,  8000] loss: 1.597\n",
            "[3,  9000] loss: 1.585\n",
            "[3, 10000] loss: 1.535\n",
            "[3, 11000] loss: 1.556\n",
            "[3, 12000] loss: 1.517\n",
            "[4,  1000] loss: 1.500\n",
            "[4,  2000] loss: 1.514\n",
            "[4,  3000] loss: 1.508\n",
            "[4,  4000] loss: 1.497\n",
            "[4,  5000] loss: 1.468\n",
            "[4,  6000] loss: 1.439\n",
            "[4,  7000] loss: 1.491\n",
            "[4,  8000] loss: 1.499\n",
            "[4,  9000] loss: 1.456\n",
            "[4, 10000] loss: 1.425\n",
            "[4, 11000] loss: 1.437\n",
            "[4, 12000] loss: 1.453\n",
            "[5,  1000] loss: 1.440\n",
            "[5,  2000] loss: 1.392\n",
            "[5,  3000] loss: 1.367\n",
            "[5,  4000] loss: 1.373\n",
            "[5,  5000] loss: 1.392\n",
            "[5,  6000] loss: 1.337\n",
            "[5,  7000] loss: 1.407\n",
            "[5,  8000] loss: 1.363\n",
            "[5,  9000] loss: 1.354\n",
            "[5, 10000] loss: 1.346\n",
            "[5, 11000] loss: 1.320\n",
            "[5, 12000] loss: 1.326\n",
            "[6,  1000] loss: 1.338\n",
            "[6,  2000] loss: 1.307\n",
            "[6,  3000] loss: 1.298\n",
            "[6,  4000] loss: 1.227\n",
            "[6,  5000] loss: 1.303\n",
            "[6,  6000] loss: 1.247\n",
            "[6,  7000] loss: 1.256\n",
            "[6,  8000] loss: 1.229\n",
            "[6,  9000] loss: 1.227\n",
            "[6, 10000] loss: 1.216\n",
            "[6, 11000] loss: 1.283\n",
            "[6, 12000] loss: 1.243\n",
            "[7,  1000] loss: 1.176\n",
            "[7,  2000] loss: 1.174\n",
            "[7,  3000] loss: 1.189\n",
            "[7,  4000] loss: 1.220\n",
            "[7,  5000] loss: 1.147\n",
            "[7,  6000] loss: 1.146\n",
            "[7,  7000] loss: 1.187\n",
            "[7,  8000] loss: 1.216\n",
            "[7,  9000] loss: 1.190\n",
            "[7, 10000] loss: 1.164\n",
            "[7, 11000] loss: 1.234\n",
            "[7, 12000] loss: 1.170\n",
            "[8,  1000] loss: 1.196\n",
            "[8,  2000] loss: 1.155\n",
            "[8,  3000] loss: 1.117\n",
            "[8,  4000] loss: 1.124\n",
            "[8,  5000] loss: 1.188\n",
            "[8,  6000] loss: 1.139\n",
            "[8,  7000] loss: 1.117\n",
            "[8,  8000] loss: 1.130\n",
            "[8,  9000] loss: 1.146\n",
            "[8, 10000] loss: 1.206\n",
            "[8, 11000] loss: 1.180\n",
            "[8, 12000] loss: 1.160\n",
            "[9,  1000] loss: 1.129\n",
            "[9,  2000] loss: 1.096\n",
            "[9,  3000] loss: 1.167\n",
            "[9,  4000] loss: 1.160\n",
            "[9,  5000] loss: 1.082\n",
            "[9,  6000] loss: 1.113\n",
            "[9,  7000] loss: 1.185\n",
            "[9,  8000] loss: 1.132\n",
            "[9,  9000] loss: 1.104\n",
            "[9, 10000] loss: 1.160\n",
            "[9, 11000] loss: 1.115\n",
            "[9, 12000] loss: 1.086\n",
            "[10,  1000] loss: 1.039\n",
            "[10,  2000] loss: 1.064\n",
            "[10,  3000] loss: 1.083\n",
            "[10,  4000] loss: 1.095\n",
            "[10,  5000] loss: 1.079\n",
            "[10,  6000] loss: 1.030\n",
            "[10,  7000] loss: 1.023\n",
            "[10,  8000] loss: 1.048\n",
            "[10,  9000] loss: 1.028\n",
            "[10, 10000] loss: 1.025\n",
            "[10, 11000] loss: 1.016\n",
            "[10, 12000] loss: 1.034\n",
            "[11,  1000] loss: 1.034\n",
            "[11,  2000] loss: 0.967\n",
            "[11,  3000] loss: 1.046\n",
            "[11,  4000] loss: 0.980\n",
            "[11,  5000] loss: 0.990\n",
            "[11,  6000] loss: 0.978\n",
            "[11,  7000] loss: 0.985\n",
            "[11,  8000] loss: 0.990\n",
            "[11,  9000] loss: 0.968\n",
            "[11, 10000] loss: 0.954\n",
            "[11, 11000] loss: 1.007\n",
            "[11, 12000] loss: 1.014\n",
            "[12,  1000] loss: 0.981\n",
            "[12,  2000] loss: 0.996\n",
            "[12,  3000] loss: 0.944\n",
            "[12,  4000] loss: 0.931\n",
            "[12,  5000] loss: 1.064\n",
            "[12,  6000] loss: 0.977\n",
            "[12,  7000] loss: 0.950\n",
            "[12,  8000] loss: 0.967\n",
            "[12,  9000] loss: 0.969\n",
            "[12, 10000] loss: 0.939\n",
            "[12, 11000] loss: 0.931\n",
            "[12, 12000] loss: 0.924\n",
            "[13,  1000] loss: 0.928\n",
            "[13,  2000] loss: 0.945\n",
            "[13,  3000] loss: 0.945\n",
            "[13,  4000] loss: 0.981\n",
            "[13,  5000] loss: 0.927\n",
            "[13,  6000] loss: 0.990\n",
            "[13,  7000] loss: 0.946\n",
            "[13,  8000] loss: 1.062\n",
            "[13,  9000] loss: 0.950\n",
            "[13, 10000] loss: 0.939\n",
            "[13, 11000] loss: 0.949\n",
            "[13, 12000] loss: 0.942\n",
            "[14,  1000] loss: 0.890\n",
            "[14,  2000] loss: 0.871\n",
            "[14,  3000] loss: 0.887\n",
            "[14,  4000] loss: 0.926\n",
            "[14,  5000] loss: 0.903\n",
            "[14,  6000] loss: 0.939\n",
            "[14,  7000] loss: 0.953\n",
            "[14,  8000] loss: 0.944\n",
            "[14,  9000] loss: 0.950\n",
            "[14, 10000] loss: 0.904\n",
            "[14, 11000] loss: 0.941\n",
            "[14, 12000] loss: 0.934\n",
            "[15,  1000] loss: 0.950\n",
            "[15,  2000] loss: 0.869\n",
            "[15,  3000] loss: 0.877\n",
            "[15,  4000] loss: 0.837\n",
            "[15,  5000] loss: 0.888\n",
            "[15,  6000] loss: 0.926\n",
            "[15,  7000] loss: 0.876\n",
            "[15,  8000] loss: 0.846\n",
            "[15,  9000] loss: 0.953\n",
            "[15, 10000] loss: 0.926\n",
            "[15, 11000] loss: 0.891\n",
            "[15, 12000] loss: 0.895\n",
            "[16,  1000] loss: 0.876\n",
            "[16,  2000] loss: 0.826\n",
            "[16,  3000] loss: 0.910\n",
            "[16,  4000] loss: 0.838\n",
            "[16,  5000] loss: 0.868\n",
            "[16,  6000] loss: 0.868\n",
            "[16,  7000] loss: 0.898\n",
            "[16,  8000] loss: 0.940\n",
            "[16,  9000] loss: 1.006\n",
            "[16, 10000] loss: 0.918\n",
            "[16, 11000] loss: 0.861\n",
            "[16, 12000] loss: 0.936\n",
            "[17,  1000] loss: 0.839\n",
            "[17,  2000] loss: 0.838\n",
            "[17,  3000] loss: 0.825\n",
            "[17,  4000] loss: 0.865\n",
            "[17,  5000] loss: 0.871\n",
            "[17,  6000] loss: 1.006\n",
            "[17,  7000] loss: 0.954\n",
            "[17,  8000] loss: 0.863\n",
            "[17,  9000] loss: 0.845\n",
            "[17, 10000] loss: 0.864\n",
            "[17, 11000] loss: 0.953\n",
            "[17, 12000] loss: 0.828\n",
            "[18,  1000] loss: 0.813\n",
            "[18,  2000] loss: 0.797\n",
            "[18,  3000] loss: 0.850\n",
            "[18,  4000] loss: 0.838\n",
            "[18,  5000] loss: 0.836\n",
            "[18,  6000] loss: 0.857\n",
            "[18,  7000] loss: 0.838\n",
            "[18,  8000] loss: 0.815\n",
            "[18,  9000] loss: 0.800\n",
            "[18, 10000] loss: 0.806\n",
            "[18, 11000] loss: 0.799\n",
            "[18, 12000] loss: 0.798\n",
            "[19,  1000] loss: 0.751\n",
            "[19,  2000] loss: 0.777\n",
            "[19,  3000] loss: 0.779\n",
            "[19,  4000] loss: 0.770\n",
            "[19,  5000] loss: 0.789\n",
            "[19,  6000] loss: 0.799\n",
            "[19,  7000] loss: 0.757\n",
            "[19,  8000] loss: 0.818\n",
            "[19,  9000] loss: 0.829\n",
            "[19, 10000] loss: 0.930\n",
            "[19, 11000] loss: 0.828\n",
            "[19, 12000] loss: 0.867\n",
            "[20,  1000] loss: 0.775\n",
            "[20,  2000] loss: 0.838\n",
            "[20,  3000] loss: 0.815\n",
            "[20,  4000] loss: 0.771\n",
            "[20,  5000] loss: 0.749\n",
            "[20,  6000] loss: 0.794\n",
            "[20,  7000] loss: 0.771\n",
            "[20,  8000] loss: 0.811\n",
            "[20,  9000] loss: 0.760\n",
            "[20, 10000] loss: 0.773\n",
            "[20, 11000] loss: 0.818\n",
            "[20, 12000] loss: 0.822\n",
            "Finished Training\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "omIuDyo5t-OA",
        "colab_type": "text"
      },
      "source": [
        "## Evaluating the transfer learning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fbZyzztJt9VO",
        "colab_type": "code",
        "outputId": "f6663168-9087-44e7-88cb-de1b733c2599",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "class_total = list(0. for i in range(len(classes)))\n",
        "class_correct = list(0. for i in range(len(classes)))\n",
        "\n",
        "for data in testloader:\n",
        "    images, labels = data\n",
        "    outputs = model(Variable(images).cuda())\n",
        "    _, predicted = torch.max(outputs.data, 1)\n",
        "    c = (predicted == labels.cuda()).squeeze()\n",
        "    \n",
        "    for i in range(len(c)):\n",
        "        label = labels[i]\n",
        "        class_correct[label] += c[i].cpu().numpy()\n",
        "        class_total[label] += 1\n",
        "\n",
        "for i in range(len(classes)):\n",
        "    print('Accuracy of {:5s} : {:5.2f} %'.format(\n",
        "        classes[i], 100 * class_correct[i] / class_total[i]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of plane : 75.40 %\n",
            "Accuracy of car   : 83.50 %\n",
            "Accuracy of bird  : 60.30 %\n",
            "Accuracy of cat   : 53.30 %\n",
            "Accuracy of deer  : 71.10 %\n",
            "Accuracy of dog   : 50.00 %\n",
            "Accuracy of frog  : 76.90 %\n",
            "Accuracy of horse : 73.20 %\n",
            "Accuracy of ship  : 77.60 %\n",
            "Accuracy of truck : 79.20 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "b8mEIylU81Tp"
      },
      "source": [
        "# Michael Nielsen book exercise of own choice\n",
        "\n",
        "**Assignment 5:** Pick an exercise of own choice from [Michael Nielsens book](http://neuralnetworksanddeeplearning.com/)\n",
        "\n",
        "**Answer:** I have chosen the following exercise from chapter 3:\n",
        "\n",
        "_Verify that $\\sigma'(z)=\\sigma(z)(1-\\sigma(z))$_:\n",
        "\n",
        "The expression for $\\sigma(z)$ is:\n",
        "$$\\sigma(z)=\\frac{1}{1+\\mathrm{e}^{-z}}$$\n",
        "Here we will use the chain rule for differentiaion:\n",
        "\\begin{align}\\sigma'(z)&=\\frac{d(1+\\mathrm{e}^{-z})^{-1}}{dz}=\\frac{-1}{(1+\\mathrm{e}^{-z})^2}\\cdot \\mathrm{e}^{-z} \\cdot (-1)\\\\&=\\sigma(z)\\cdot\\frac{\\mathrm{e}^{-z}}{1+\\mathrm{e}^{-z}}=\\sigma(z)\\bigg(1-\\frac{1}{1+\\mathrm{e}^{-z}}\\bigg)\\\\ &=\\sigma(z)(1-\\sigma(z))\\end{align}\n",
        "\n",
        "\n"
      ]
    }
  ]
}