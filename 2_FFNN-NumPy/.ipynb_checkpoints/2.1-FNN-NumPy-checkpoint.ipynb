{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contents and why we need this lab\n",
    "\n",
    "This lab is about implementing neural networks yourself in NumPy before we start using other frameworks which hide some of the computation from you. It builds on the first lab where you derived the equations for neural network forward and backward propagation and gradient descent parameter updates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# External sources of information\n",
    "\n",
    "1. Jupyter notebook. You can find more information about Jupyter notebooks [here](https://jupyter.org/). It will come as part of the [Anaconda](https://www.anaconda.com/) Python installation. \n",
    "2. [NumPy](https://numpy.org/). Part of Anaconda distribution. If you already know how to program most things about Python and NumPy can be found through Google search. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This notebook will follow the next steps:\n",
    "\n",
    "1. Data generation\n",
    "2. Initialization of parameters\n",
    "3. Definition of activation functions   \n",
    "4. A short explanation of numpy's einsum function\n",
    "5. Forward pass\n",
    "6. Backward pass (backward pass and finite differences)\n",
    "7. Training loop \n",
    "8. Testing your model\n",
    "9. Further extensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create an artificial dataset to play with\n",
    "\n",
    "We create a non-linear 1d regression task. The generator has support various noise levels and it creates train, validation and test sets. You can modify it yourself if you want more or less challenging tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator(noise=0.1, n_samples=300, D1=True):\n",
    "    # Create covariates and response variable\n",
    "    if D1:\n",
    "        X = np.linspace(-3, 3, num=n_samples).reshape(-1,1) # 1-D\n",
    "        np.random.shuffle(X)\n",
    "        y = np.random.normal((0.5*np.sin(X[:,0]*3) + X[:,0]), noise) # 1-D with trend\n",
    "    else:\n",
    "        X = np.random.multivariate_normal(np.zeros(3), noise*np.eye(3), size = n_samples) # 3-D\n",
    "        np.random.shuffle(X)    \n",
    "        y = np.sin(X[:,0]) - 5*(X[:,1]**2) + 0.5*X[:,2] # 3-D\n",
    "\n",
    "    # Stack them together vertically to split data set\n",
    "    data_set = np.vstack((X.T,y)).T\n",
    "    \n",
    "    train, validation, test = np.split(data_set, [int(0.35*n_samples), int(0.7*n_samples)], axis=0)\n",
    "    \n",
    "    # Standardization of the data, remember we do the standardization with the training set mean and standard deviation\n",
    "    train_mu = np.mean(train, axis=0)\n",
    "    train_sigma = np.std(train, axis=0)\n",
    "    \n",
    "    train = (train-train_mu)/train_sigma\n",
    "    validation = (validation-train_mu)/train_sigma\n",
    "    test = (test-train_mu)/train_sigma\n",
    "    \n",
    "    x_train, x_validation, x_test = train[:,:-1], validation[:,:-1], test[:,:-1]\n",
    "    y_train, y_validation, y_test = train[:,-1], validation[:,-1], test[:,-1]\n",
    "\n",
    "    return x_train, y_train,  x_validation, y_validation, x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "D1 = True\n",
    "x_train, y_train,  x_validation, y_validation, x_test, y_test = data_generator(noise=0.5, D1=D1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "if D1:\n",
    "    plt.scatter(x_train[:,0], y_train);\n",
    "    plt.scatter(x_validation[:,0], y_validation);\n",
    "    plt.scatter(x_test[:,0], y_test);\n",
    "else:\n",
    "    plt.scatter(x_train[:,1], y_train);\n",
    "    plt.scatter(x_validation[:,1], y_validation);\n",
    "    plt.scatter(x_test[:,1], y_test);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The steps to create a feed forward neural network are the following:\n",
    "\n",
    "1. **Number of hidden layer and hidden units**. We have to define the number of hidden units in each layer. We are going to save these numbers in a list \"L\" that is going to start with our input dimensionality (the number of features in X) and is going to finish with our output dimensionality (the size of Y). Anything in between these values are going to be hidden layers and the number of hidden units in each hidden layer is defined by the researcher. Remember that for each unit in each layer (besides the first one, according to our list L) there is a bias term.\n",
    "2. **Activation functions** for each hidden layer. Each hidden layer in your list must have an activation function (it can also be the linear activation which is equivalent to identity function). The power of neural networks comes from non-linear activation functions that learn representations (features) from the data allowing us to learn from it. \n",
    "3. **Parameter initialization**. We will initialize the weights to have random values. This is done in practice by drawing pseudo random numbers from a Gaussian or uniform distribution. It turns out that for deeper models we have to be careful about how we scale the random numbers. This will be the topic of the exercise below. For now we will just use unit variance Gaussians.  \n",
    "\n",
    "Our initialization will work as follows: \n",
    "\n",
    "For each layer of the neural network defined in L, initialize a matrix of weights of size (units_in, units_out) from a random normal distribution [np.random.normal()](https://docs.scipy.org/doc/numpy-1.15.0/reference/generated/numpy.random.normal.html) and save them in a list called \"layers\". For each layer in our neural network, initialize a matrix of weights of size (1, units_out) as above and save them in a list called \"bias\". The function should return a tuple (layers, bias). The length of our lists must be len(L)-1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialize neural network:\n",
    "# the NN is a tuple with a list with weights and list with biases\n",
    "def init_NN(L):\n",
    "    \"\"\"\n",
    "    Function that initializes our feed-forward neural network. \n",
    "    Input: \n",
    "    L: list of integers. The first element must be of the size of number of features (dimension) of x and the last element \n",
    "        must be the size of the number of outputs.\n",
    "    Output:\n",
    "    A tuple of:\n",
    "    weights: a list with randomly initialized weights of shape (in units, out units) each. The units are the ones we defined in L.\n",
    "        For example, if L = [2, 3, 4] layers must be a list with a first element of shape (2, 3) and a second elemtn of shape (3, 4). \n",
    "        The length of layers must be len(L)-1\n",
    "    biases: a list with randomly initialized biases of shape (1, out_units) each. For the example above, bias would be a list of length\n",
    "        2 with a first element of shape (1, 3) and a second element of shape (1, 4).\n",
    "    \"\"\"\n",
    "    weights = []\n",
    "    biases  = []\n",
    "    for i in range(len(L)-1):\n",
    "        weights.append(np.random.normal(loc=0.0, scale=1.0, size=[L[i],L[i+1]])) \n",
    "        biases.append(np.random.normal(loc=0.0, scale=1.0, size=[1, L[i+1]]))     \n",
    "        \n",
    "    return (weights, biases)\n",
    "\n",
    "# Initialize the unit test neural network:\n",
    "# Same steps as above but we will not initialize the weights randomly.\n",
    "def init_NN_UT(L):\n",
    "    weights = []\n",
    "    biases  = []\n",
    "    for i in range(len(L)-1):\n",
    "        weights.append(np.ones((L[i],L[i+1]))) \n",
    "        biases.append(np.ones((1, L[i+1])))     \n",
    "        \n",
    "    return (weights, biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializer the unit test neural network\n",
    "L_UT  = [3, 5, 1]\n",
    "NN_UT = init_NN_UT(L_UT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1.]]\n",
      "[[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n"
     ]
    }
   ],
   "source": [
    "for x in NN_UT[0]:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 1. 1. 1. 1.]]\n",
      "[[1.]]\n"
     ]
    }
   ],
   "source": [
    "for x in NN_UT[1]:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise a) Print all network parameters\n",
    "\n",
    "Make a function that prints all parameters (weights and biases) with information about in which layer the parameters are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def summary(model):\n",
    "    ws = model[0]\n",
    "    bs = model[1]\n",
    "    \n",
    "    print('WEIGHTS:')\n",
    "    print('-'*10)\n",
    "    for i,w in enumerate(ws):\n",
    "        print('Layer', i+1)\n",
    "        print(w)\n",
    "        print('')\n",
    "    print('')\n",
    "    \n",
    "    print('BIASES:')\n",
    "    print('-'*10)\n",
    "    for i,b in enumerate(bs):\n",
    "        print('Layer', i+1)\n",
    "        print(b)\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WEIGHTS:\n",
      "----------\n",
      "Layer 1\n",
      "[[1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1.]]\n",
      "\n",
      "Layer 2\n",
      "[[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "\n",
      "\n",
      "BIASES:\n",
      "----------\n",
      "Layer 1\n",
      "[[1. 1. 1. 1. 1.]]\n",
      "\n",
      "Layer 2\n",
      "[[1.]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "summary(NN_UT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced initialization schemes\n",
    "\n",
    "If we are not careful with initialization we can run into trouble with in both the forward and backward passes. We have random weights with random +/- sign so the signal we pass forward will also be random and zero on average. However, the absolute size of the signal may grow or shrink from layer to layer depending upon the absolute scale of random weights. A statistical analysis of this effect and the same effect for the backward pass are presented in these two papers: [Glorot initialization](http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf) and [He initialization](https://arxiv.org/pdf/1502.01852v1.pdf). \n",
    "\n",
    "The result of the analyses are proposals for how to make the initialization such that the variance of the signals (forward and backward) are kept constant when propagating layer to layer. The exact expressions depend upon the activation function used.\n",
    "\n",
    "We define $n_{in}$ and $n_{out}$ as the number of input units and output units of a particular layer. \n",
    "\n",
    "In the linked paper, Glorot and Bengio propose that for tanh activation functions the following two alternative initializations:\n",
    "\n",
    "$$w_{ij} \\sim U \\bigg[ -\\sqrt{\\frac{6}{(n_{in} + n_{out})}}, \\, \\sqrt{\\frac{6}{(n_{in} + n_{out})}} \\bigg]$$\n",
    "\n",
    "$$w_{ij} \\sim N \\bigg( 0, \\, \\frac{2}{(n_{in} + n_{out})} \\bigg) \\ . $$\n",
    "\n",
    "Here $U[a,b]$ is a uniform distribution in the interval $a$ to $b$ and $N(\\mu,\\sigma^2)$ is a Gaussian distribution with mean $\\mu$ and variance $\\sigma^2$.\n",
    "\n",
    "He et.al. proposes for Rectified Linear Unit activations (ReLU) the following initialization:\n",
    "\n",
    "$$w_{ij} \\sim U \\bigg[ -\\sqrt{\\frac{6}{n_{in}}}, \\, \\sqrt{\\frac{6}{n_{in}}} \\bigg]$$\n",
    "\n",
    "$$w_{ij} \\sim N \\bigg( 0, \\, \\frac{2}{n_{in}} \\bigg) \\ . $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise b) Glorot and He initialization\n",
    "\n",
    "Implement these initialization schemes by modifying the code given below.\n",
    "\n",
    "**NOTE:** The Gaussian is defined as $N( \\mu, \\, \\sigma^{2})$ but Numpy takes $\\sigma$ as argument.\n",
    "\n",
    "Explain briefly how you would test numerically that these initializations have the sought after property. Hint: See plots in Glorot paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Glorot\n",
    "def init_NN_glorot_Tanh(L, uniform=False):\n",
    "    \"\"\"\n",
    "    Initializer using the glorot initialization scheme\n",
    "    \"\"\"\n",
    "    weights = []\n",
    "    biases  = []\n",
    "    for i in range(len(L)-1):\n",
    "        n_in, n_out = L[i], L[i+1]\n",
    "        if uniform:\n",
    "            bound = np.sqrt(6 / (n_in + n_out))\n",
    "            weights.append(np.random.uniform(low=-bound, high=bound, size=[n_in, n_out])) \n",
    "            biases.append(np.random.uniform(low=-bound, high=bound, size=[1, n_out]))  \n",
    "        else:\n",
    "            std = 2 / (n_in + n_out)\n",
    "            weights.append(np.random.normal(loc=0.0, scale=std, size=[n_in, n_out])) \n",
    "            biases.append(np.random.normal(loc=0.0, scale=std, size=[1, n_out]))       \n",
    "        \n",
    "    return (weights, biases)\n",
    "\n",
    "## He\n",
    "def init_NN_he_ReLU(L, uniform=False):\n",
    "    \"\"\"\n",
    "    Initializer using the He initialization scheme\n",
    "    \"\"\"\n",
    "    weights = []\n",
    "    biases  = []\n",
    "    for i in range(len(L)-1):\n",
    "        n_in, n_out = L[i], L[i+1]\n",
    "        if uniform:\n",
    "            bound = np.sqrt(6 / n_in)\n",
    "            weights.append(np.random.uniform(low=-bound, high=bound, size=[n_in, n_out])) \n",
    "            biases.append(np.random.uniform(low=-bound, high=bound, size=[1, n_out]))  \n",
    "        else:\n",
    "            std = 2 / n_in\n",
    "            weights.append(np.random.normal(loc=0.0, scale=std, size=[n_in, n_out])) \n",
    "            biases.append(np.random.normal(loc=0.0, scale=std, size=[1, n_out]))\n",
    "        \n",
    "    return (weights, biases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer to (b)\n",
    "According to figure 6 in the Glorot paper, the activations in the different layers will have a similar standard deviation. For standard initializations the activations will have different standard deviations, with the std. deviation decreasing for each layer, meaning the first layer will be very spread out and the last layer will be concentrated around the mean."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activation functions\n",
    "\n",
    "To have a full definition of the neural network, we must define an activation function for every layer in our list L (again, exluding the first term, which is the number of input dimensions). Several activation functions have been proposed and have different characteristics. Here, we will implement the linear activation function (the idenity function), the sigmoid activation function (squeeshing the outcome of each neuron into the $[0, 1]$ range) the Hyperbolic Tangent (Tanh) that squeeshes the outcome of each neuron to $[-1, 1]$ and the Rectified Linear Unit (ReLU). \n",
    "\n",
    "We will also include the derivative in the function. We need this in order to do our back-propagation algorithm. Don't rush, we will get there soon. For any neural network, save the activation functions in a list. This list must be of size len(L)-1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Linear(x, derivative=False):\n",
    "    \"\"\"\n",
    "    Computes the element-wise Linear activation function for an array x\n",
    "    inputs:\n",
    "    x: The array where the function is applied\n",
    "    derivative: if set to True will return the derivative instead of the forward pass\n",
    "    \"\"\"\n",
    "    \n",
    "    if derivative:              # Return the derivative of the function evaluated at x\n",
    "        return np.ones_like(x)\n",
    "    else:                       # Return the forward pass of the function at x\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sigmoid activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Sigmoid(x, derivative=False):\n",
    "    \"\"\"\n",
    "    Computes the element-wise Sigmoid activation function for an array x\n",
    "    inputs:\n",
    "    x: The array where the function is applied\n",
    "    derivative: if set to True will return the derivative instead of the forward pass\n",
    "    \"\"\"\n",
    "    f = 1/(1+np.exp(-x))\n",
    "    \n",
    "    if derivative:              # Return the derivative of the function evaluated at x\n",
    "        return f*(1-f)\n",
    "    else:                       # Return the forward pass of the function at x\n",
    "        return f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperbolic Tangent activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Tanh(x, derivative=False):\n",
    "    \"\"\"\n",
    "    Computes the element-wise Sigmoid activation function for an array x\n",
    "    inputs:\n",
    "    x: The array where the function is applied\n",
    "    derivative: if set to True will return the derivative instead of the forward pass\n",
    "    \"\"\"\n",
    "    f = (np.exp(x)-np.exp(-x))/(np.exp(x)+np.exp(-x))\n",
    "    \n",
    "    if derivative:              # Return the derivative of the function evaluated at x\n",
    "        return 1-f**2\n",
    "    else:                       # Return the forward pass of the function at x\n",
    "        return f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rectifier linear unit (ReLU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ReLU(x, derivative=False):\n",
    "    \"\"\"\n",
    "    Computes the element-wise Rectifier Linear Unit activation function for an array x\n",
    "    inputs:\n",
    "    x: The array where the function is applied\n",
    "    derivative: if set to True will return the derivative instead of the forward pass\n",
    "    \"\"\"\n",
    "    \n",
    "    if derivative:              # Return the derivative of the function evaluated at x\n",
    "        return (x>0).astype(int)\n",
    "    else:                       # Return the forward pass of the function at x\n",
    "        return np.maximum(x, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise c) Glorot initialization for all activation functions\n",
    "\n",
    "Implement a function by adding to the code snippet below that can take network L and list of activations function as argument and return Glorot initialized network.  Hint: [This blog post](https://mmuratarat.github.io/2019-02-25/xavier-glorot-he-weight-init) gives a table for the activation functions we use here.\n",
    "\n",
    "Briefly explain in words how these how these values are calculated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer for (c)\n",
    "We iterate through the pairs of number of units per layer and based on the activation function connecting that pair of layer we initialize the weights in that layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_NN_Glorot(L, activations, uniform=False):\n",
    "    \"\"\"\n",
    "    Initializer using the glorot initialization scheme\n",
    "    \"\"\"\n",
    "    weights, biases = [], []\n",
    "    \n",
    "    for i in range(len(L)-1):\n",
    "        units = [L[i], L[i+1]]\n",
    "        a = activations[i]\n",
    "        \n",
    "        if a == ReLU:\n",
    "            layer = init_NN_he_ReLU(units)\n",
    "        elif a == Tanh:\n",
    "            layer = init_NN_glorot_Tanh(units)\n",
    "        else: \n",
    "            layer = init_NN(units)\n",
    "        \n",
    "        weights.append(layer[0])\n",
    "        biases.append(layer[1])\n",
    "    \n",
    "    return weights, biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([[array([[ 0.92572151, -1.7729009 , -1.38558444, -0.82709339, -0.78501891],\n",
       "          [ 0.1358012 ,  0.19322818, -0.16988085, -0.09991864,  0.35184317],\n",
       "          [ 0.55548259, -0.88435934, -0.24126065, -0.04639322,  0.323544  ]])],\n",
       "  [array([[ 0.03291526],\n",
       "          [-0.04344838],\n",
       "          [ 0.16682584],\n",
       "          [-0.35108624],\n",
       "          [-0.11689384]])]],\n",
       " [[array([[-0.51511143, -0.05948078,  0.01998909, -0.46964674, -0.48554343]])],\n",
       "  [array([[-0.07640535]])]])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initializer the unit test neural network\n",
    "L_UT  = [3, 5, 1]\n",
    "ACT_Glorot = [ReLU, Tanh]\n",
    "NN_Glorot = init_NN_Glorot(L_UT, ACT_Glorot)\n",
    "\n",
    "NN_Glorot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numpy einsum (EINstein SUMmation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Einsum](https://docs.scipy.org/doc/numpy/reference/generated/numpy.einsum.html) gives us the possibility to compute almost any matrix operation in a single function. You can find a good description in the link above. Here are a few examples of some important uses:\n",
    "\n",
    "**Transpose:** We can write the transpose of matrix $A$:\n",
    "\n",
    "```\n",
    "np.einsum('ij -> ji', A) \n",
    "```\n",
    "\n",
    "**Trace:** We can write the trace of matrix $A$:\n",
    "\n",
    "```\n",
    "np.einsum('ii -> ', A) \n",
    "```\n",
    "\n",
    "**Diagonal:** We can write the diagonal of matrix $A$:\n",
    "\n",
    "```\n",
    "np.einsum('ii -> i', A) \n",
    "```\n",
    " \n",
    "**Matrix product:** We can write the multiplication of matrices $A$ and $B$ as:\n",
    "\n",
    "```\n",
    "np.einsum('ij, jk -> ik', A, B)\n",
    "```\n",
    "\n",
    "Note that $j$ in both matrices $A$ and $B$ should be the same size. \n",
    "\n",
    "**Batched matrix product (or why bothering):** All of the functions we performed above are built in numpy (np.tranpose, np.trace, np.matmul), however, when you want to do more complex operations, it might become less readable and computationaly efficient. Let's introduce a three dimensional matrix $H$ with indices $b,j,k$, where the first dimension is the batch (training example) dimension. In einsum, we can then write:\n",
    "\n",
    "```\n",
    "np.einsum('ij, bjk -> bik', A, H)\n",
    "```\n",
    "\n",
    "In order to perform a batched matrix multiplication where we multiple over the second dimension in the first marix and second dimension in the second matrix. The result is a new three dimensional matrix where the first dimension is the first dimension from $H$ and second is the first dimension from $A$ and last dimension the last dimension from $H$. This is a very simple one line (and readable) way to do matrix operations that will be very useful for neural network code. \n",
    "\n",
    "\n",
    "#### _**Tips and tricks when using einsum**_\n",
    "\n",
    "At the beginning, einsum might be a bit difficult to work with. The most important thing to do when using it is keeping track of the dimensions of your input and output matrices. An easy way to keep track of these dimensions is by using some sort of naming convention. Just like in the batched matrix product above we used $b$ to denote the batch dimension. In all the functions of this notebook, we leave some convention of names of indexes for the einsum in the explanation of the functions. We hope you find them useful!\n",
    "\n",
    "There are some other useful resources to understand numpy.einsum:\n",
    "\n",
    "* [Olexa Bilaniuk's great blogpost on einsum]( https://obilaniu6266h16.wordpress.com/2016/02/04/einstein-summation-in-numpy/ )\n",
    "* [Stackoverflow answer to: Understanding NumPy's einsum]( https://stackoverflow.com/q/26089893/8899404 )\n",
    "* [Jessica Stringham post on einsum]( https://jessicastringham.net/2018/01/01/einsum/ )\n",
    "* [Slides of einstein summation from oxford]( http://www-astro.physics.ox.ac.uk/~sr/lectures/vectors/lecture10final.pdfc )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward pass\n",
    "\n",
    "The forward pass has been implemented for you. Please note how we have used einsum to perform the affine tranformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def forward_pass(x, NN, activations):\n",
    "    \"\"\"\n",
    "    This functions performs a forward pass recursively. It saves lists for both affine transforms of units (z) and activated units (a)\n",
    "    Input:\n",
    "    x: The input of the network             (np.array of shape: (batch_size, number_of_features))\n",
    "    NN: The initialized neural network      (tuple of list of matrices)\n",
    "    activations: the activations to be used (list of functions, same len as NN)\n",
    "\n",
    "    Output:\n",
    "    a: A list of affine transformations, that is, all x*w+b.\n",
    "    z: A list of activated units (ALL activated units including input and output).\n",
    "    \n",
    "    Shapes for the einsum:\n",
    "    b: batch size\n",
    "    i: size of the input hidden layer (layer l)\n",
    "    o: size of the output (layer l+1)\n",
    "    \"\"\"\n",
    "    z = [x]\n",
    "    a = []\n",
    "        \n",
    "    for l in range(len(NN[0])):\n",
    "        a.append(np.einsum('bi, io -> bo', z[l], NN[0][l]) + NN[1][l])  # The affine transform x*w+b\n",
    "        z.append(activations[l](a[l]))                                  # The non-linearity    \n",
    "    \n",
    "    return a, z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward pass unit test\n",
    "\n",
    "Below is a piece of code that takes a very particular setting of the network and inputs and test whether it gives the expected results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "ACT_F_UT = [Linear, Linear]\n",
    "test_a, test_z = forward_pass(np.array([[1,1,1]]), NN_UT, ACT_F_UT) # input has shape (1, 3) 1 batch, 3 features\n",
    "\n",
    "# Checking shapes consistency\n",
    "assert np.all(test_z[0]==np.array([1,1,1])) # Are the input vector and the first units the same?\n",
    "assert np.all(test_z[1]==test_a[0])         # Are the first affine transformations and hidden units the same?\n",
    "assert np.all(test_z[2]==test_a[1])         # Are the output units and the affine transformations the same?\n",
    "\n",
    "# Checking correctnes of values\n",
    "# First layer, calculate np.sum(np.array([1,1,1])*np.array([1,1,1]))+1 = 4\n",
    "assert np.all(test_z[1] == 4.)\n",
    "# Second layer, calculate np.sum(np.array([4,4,4,4,4])*np.array([1,1,1,1,1]))+1 = 21\n",
    "assert np.all(test_z[2] == 21.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to perform a backward pass we need to define a loss function and its derivative with respect to the output of the neural network $y$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def squared_error(t, y, derivative=False):\n",
    "    \"\"\"\n",
    "    Computes the squared error function and its derivative \n",
    "    Input:\n",
    "    t:      target (expected output)          (np.array)\n",
    "    y:      output from forward pass (np.array, must be the same shape as t)\n",
    "    derivative: whether to return the derivative with respect to y or return the loss (boolean)\n",
    "    \"\"\"\n",
    "    if np.shape(t)!=np.shape(y):\n",
    "        print(\"t and y have different shape\")\n",
    "    if derivative: # Return the derivative of the function\n",
    "        return (y-t)\n",
    "    else:\n",
    "        return 0.5*(y-t)**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise d) Implement cross entropy loss\n",
    "\n",
    "Insert code below to implement cross-entropy loss for general dimensionality of $t$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cross_entropy_loss(t, y, derivative=False):\n",
    "    \"\"\"\n",
    "    Computes the cross entropy loss function and its derivative \n",
    "    Input:\n",
    "    t:      target (expected output)          (np.array)\n",
    "    y:      output from forward pass (np.array, must be the same shape as t)\n",
    "    derivative: whether to return the derivative with respect to y or return the loss (boolean)\n",
    "    \"\"\"\n",
    "    if np.shape(t)!=np.shape(y):\n",
    "        print(\"t and y have different shape\")\n",
    "    if derivative: # Return the derivative of the function\n",
    "        return t * (1 / y)\n",
    "    else:\n",
    "        return t * np.log(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backward pass "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise e) Complete code for backward pass\n",
    "\n",
    "Below is a implementation of the backward pass with some lines removed. Insert the missing lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def backward_pass(x, t, y, z, a, NN, activations, loss):\n",
    "    \"\"\"\n",
    "    This function performs a backward pass ITERATIVELY. It saves lists all of the derivatives in the process\n",
    "    \n",
    "    Input:\n",
    "    x:           The input used for the batch                (np.array)\n",
    "    t:           The observed targets                        (np.array, the first dimension must be the same to x)\n",
    "    y:           The output of the forward_pass of NN for x  (np.array, must have the same shape as t)\n",
    "    a:           The affine transforms from the forward_pass (np.array)\n",
    "    z:           The activated units from the forward_pass   (np.array)\n",
    "    activations: The activations to be used                  (list of functions)\n",
    "    loss:        The loss function to be used                (one function)\n",
    "    \n",
    "    Output:\n",
    "    g_w: A list of gradients for every hidden unit \n",
    "    g_b: A list of gradients for every bias\n",
    "    \n",
    "    Shapes for the einsum:\n",
    "    b: batch size\n",
    "    i: size of the input hidden layer (layer l)\n",
    "    o: size of the output (layer l+1)\n",
    "    \"\"\"\n",
    "    BS = x.shape[0] # Implied batch shape \n",
    "    \n",
    "    # First, let's compute the list of derivatives of z with respect to a \n",
    "    d_a = []\n",
    "    for i in range(len(activations)):\n",
    "        d_a.append(activations[i](a[i], derivative=True))\n",
    "    \n",
    "    # Second, let's compute the derivative of the loss function\n",
    "    t = t.reshape(BS, -1)\n",
    "    \n",
    "    d_loss = loss(t, y, derivative=True)\n",
    "    \n",
    "     \n",
    "    # Third, let's compute the derivative of the biases and the weights\n",
    "    g_w   = [] # List to save the gradient of the weights\n",
    "    g_b   = [] # List to save the gradients of the biases\n",
    "\n",
    "    delta = np.einsum('bo, bo -> bo', d_loss, d_a[-1]) # loss shape: (b, o); pre-activation units shape: (b, o) hadamard product\n",
    "\n",
    "    g_b.append(np.mean(delta, axis=0))\n",
    "    g_w.append(np.mean(np.einsum('bo, bi -> bio', delta, z[-2]), axis=0)) # delta shape: (b, o), activations shape: (b, h) \n",
    "    \n",
    "    n_layers = len(NN[0])\n",
    "    \n",
    "    for l in range(1, n_layers):\n",
    "        d_C_d_z = np.einsum('bo, io -> bi', delta, NN[0][-l])  # Derivative of the Cost with respect to an activated layer d_C_d_z. \n",
    "                                                               # delta shape: as above; weights shape: (i, o)\n",
    "                                                               # Delta: d_C_d_z (element-wise mult) derivative of the activation layers\n",
    "                                                               # delta shape: as above; d_z shape: (b, i)  \n",
    "        delta = 0   # <- Insert correct expression\n",
    "                                                                \n",
    "        g_b.append(np.mean(delta, axis=0)) \n",
    "        g_w.append(np.mean(np.einsum('bo, bi -> bio', delta, z[-l-2]), axis=0)) # Derivative of cost with respect to weights in layer l:\n",
    "                                                                                # delta shape: as above; activations of l-1 shape: (b, i)\n",
    "    \n",
    "    return g_b[::-1], g_w[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4, 5, 6, 7, 8])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backward pass unit test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to perform the unit test of the backward pass with a finite difference estimation, make sure to read the description of the function and that you understand it well:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise f) Test correctness of derivatives with finite difference method\n",
    "\n",
    "Write a small function that uses [the finite difference method](https://en.wikipedia.org/wiki/Finite_difference_method) do test whether the backpropation implementation is working. In short we will use\n",
    "$$\n",
    "\\frac{\\partial E(w)}{\\partial w_{ij}^{(l)}} \\approx \\frac{E(v)-E(w)}{dw}\n",
    "$$\n",
    "for $dw$ is much smaller than one and $v$ is the same network as $w$ apart from $v_{ij}^{(l)} = w_{ij}^{(l)} + dw$.\n",
    "\n",
    "As arguments the function should take: some data $x$ and $t$ as in the example above, the network including activations, the indices $i$, $j$, $l$ of the weight we investigate and $dw$ and return the right hand side of the expression above.\n",
    "\n",
    "_Insert your code in the cell below._\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Insert your finite difference code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have implemented the function you can compare this number with the left hand side computed by the implementation above.\n",
    "\n",
    "Try for different parameters and different values of $dw$. Scan over a range of $dw$ values. Why does the method break dow for really small $dw$?\n",
    "\n",
    "_Insert your written answer here._\n",
    "\n",
    "Finite differences gives us gradients without computing gradients explicitly. Why don't we use it in practice then?\n",
    "\n",
    "_Insert your written answer here._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is reference code that computes the finite differences for all parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def finite_diff_grad(x, NN, ACT_F, epsilon=None):\n",
    "    \"\"\"\n",
    "    Finite differences gradient estimator: https://en.wikipedia.org/wiki/Finite_difference_method\n",
    "    The idea is that we can approximate the derivative of any function (f) with respect to any argument (w) by evaluating the function at (w+e)\n",
    "    where (e) is a small number and then computing the following opertion (f(w+e)-f(w))/e . Note that we would need N+1 evaluations of\n",
    "    the function in order to compute the whole Jacobian (first derivatives matrix) where N is the number of arguments. The \"+1\" comes from the\n",
    "    fact that we also need to evaluate the function at the current values of the argument.\n",
    "    \n",
    "    Input:\n",
    "    x:       The point at which we want to evaluate the gradient\n",
    "    NN:      The tuple that contains the neural network\n",
    "    ACT_F:   The activation functions in order to perform the forward pass\n",
    "    epsilon: The size of the difference\n",
    "    \n",
    "    Output:\n",
    "    Two lists, the first one contains the gradients with respect to the weights, the second with respect to the biases\n",
    "    \"\"\"\n",
    "    from copy import deepcopy\n",
    "    \n",
    "    if epsilon == None:\n",
    "        epsilon = np.finfo(np.float32).eps # Machine epsilon for float 32\n",
    "        \n",
    "    grads = deepcopy(NN)               # Copy of structure of the weights and biases to save the gradients                        \n",
    "    test_a, _ = forward_pass(x, NN_UT, ACT_F_UT) # We evaluate f(x)\n",
    "    \n",
    "    for e in range(len(NN)):                       # Iterator over elements of the NN:       weights or biases\n",
    "        for h in range(len(NN[e])):                # Iterator over the layer of the element: layer number\n",
    "            for r in range(NN[e][h].shape[0]):     # Iterator over                           row number\n",
    "                for c in range(NN[e][h].shape[1]): # Iterator over                           column number \n",
    "                    NN_copy             = deepcopy(NN)    \n",
    "                    NN_copy[e][h][r,c] += epsilon\n",
    "                    test_a_eps, _       = forward_pass(x, NN_copy, ACT_F)     # We evaluate f(x+eps)\n",
    "                    grads[e][h][r,c]    = (test_a_eps[-1]-test_a[-1])/epsilon # Definition of finite differences gradient\n",
    "    \n",
    "    return grads[0], grads[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### Unit test \n",
    "\n",
    "## First lest's compute the backward pass using our own function\n",
    "# Forward pass\n",
    "test_a, test_z = forward_pass(np.array([[1,1,1]]), NN_UT, ACT_F_UT)\n",
    "# Backward pass\n",
    "test_g_b, test_g_w = backward_pass(np.array([[1,1,1]]), np.array([20]), test_a[-1], test_z, test_a, NN_UT, ACT_F_UT, squared_error)\n",
    "# Estimation by finite differences\n",
    "test_fdg_w, test_fdg_b = finite_diff_grad(np.array([[1,1,1]]), NN_UT, ACT_F_UT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Test whether the weights and biases are all equal as the ones we estimated using back propagation\n",
    "for l in range(len(test_g_w)):\n",
    "    assert np.allclose(test_fdg_w[l], test_g_w[l])\n",
    "    assert np.allclose(test_fdg_b[l], test_g_b[l])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and validation\n",
    "\n",
    "We are ready to train some neural networks! Below we give some example initializations and a training loop. Try it out. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialize an arbitrary neural network\n",
    "#L  = [3, 16, 1]\n",
    "L  = [1, 8, 1]\n",
    "NN = init_NN(L)\n",
    "#NN = init_NN_glorot(L, uniform=True)\n",
    "#NN = init_NN_he_ReLU(L, uniform=True)\n",
    "\n",
    "ACT_F = [ReLU, Linear]\n",
    "#ACT_F = [Tanh, Linear]\n",
    "\n",
    "# Recommended hyper-parameters for 1-D: \n",
    "# L  = [1, 8, 1]\n",
    "# EPOCHS = 10000\n",
    "# BATCH_SIZE = 128 \n",
    "# LEARN_R = 2.5e-1 for Tanh and LEARN_R = 1e-1 for ReLU\n",
    "\n",
    "# Recommended hyper-parameters for 3-D: \n",
    "# L  = [3, 16, 1] \n",
    "# EPOCHS = 10000\n",
    "# BATCH_SIZE = 128 \n",
    "# LEARN_R = 5e-2 for ReLU and LEARN_R = 1e-1 for Tanh\n",
    "\n",
    "### Notice that, when we switch from tanh to relu activation, we decrease the learning rate. This is due the stability of the gradients \n",
    "## of the activation functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialize training hyperparameters\n",
    "EPOCHS = 20000\n",
    "BATCH_SIZE = 128 \n",
    "LEARN_R = 1e-2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_loss = []\n",
    "val_loss = []\n",
    "\n",
    "for e in range(EPOCHS):\n",
    "    # Mini-batch indexes\n",
    "    idx = np.random.choice(x_train.shape[0], size=BATCH_SIZE)\n",
    "    # Forward pass\n",
    "    aff, units = forward_pass(x_train[idx,:], NN, ACT_F)\n",
    "    # Backward pass\n",
    "    g_b, g_w = backward_pass(x_train[idx,:], y_train[idx], units[-1], units, aff, NN, ACT_F, squared_error)\n",
    "    \n",
    "    # Stochastic gradient descent\n",
    "    for l in range(len(g_b)):\n",
    "        NN[0][l] -= LEARN_R*g_w[l]\n",
    "        NN[1][l] -= LEARN_R*g_b[l]\n",
    "        \n",
    "    # Training loss\n",
    "    _, units = forward_pass(x_train, NN, ACT_F)\n",
    "    # Estimate loss function\n",
    "    #print(np.max(squared_error(y_train, units[-1])))\n",
    "    train_loss.append(np.mean(squared_error(y_train, np.squeeze(units[-1]))))\n",
    "    \n",
    "    # Validation\n",
    "    # Forward pass\n",
    "    _, units = forward_pass(x_validation, NN, ACT_F)\n",
    "    # Estimate validation loss function\n",
    "    val_loss.append(np.mean(squared_error(y_validation, np.squeeze(units[-1]))))\n",
    "    \n",
    "    if e%500==0:\n",
    "        print(\"{:4d}\".format(e),\n",
    "              \"({:5.2f}%)\".format(e/EPOCHS*100), \n",
    "              \"Train loss: {:4.3f} \\t Validation loss: {:4.3f}\".format(train_loss[-1], val_loss[-1]))\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(len(train_loss)), train_loss);\n",
    "plt.plot(range(len(val_loss)), val_loss);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing\n",
    "\n",
    "We have kept the calculation of the test error separate in order to emphasize that Separate code for testing to empahsize not to use test set in optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, units = forward_pass(x_test, NN, ACT_F)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(y_test, units[-1]);\n",
    "plt.plot([np.min(y_test), np.max(y_test)], [np.min(y_test), np.max(y_test)], color='k');\n",
    "plt.xlabel(\"y\");\n",
    "plt.ylabel(\"$\\hat{y}$\");\n",
    "plt.title(\"Model prediction vs real in the test set, the close to the line the better\")\n",
    "plt.grid(True);\n",
    "plt.axis('equal');\n",
    "plt.tight_layout();\n",
    "\n",
    "print(\"Test loss:  {:4.3f}\".format(np.mean(squared_error(y_test, np.squeeze(units[-1])))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if D1:\n",
    "    plt.scatter(x_train[:,0], y_train, label=\"train data\");\n",
    "    plt.scatter(x_test[:,0], units[-1], label=\"test prediction\");\n",
    "    plt.scatter(x_test[:,0], y_test, label=\"test data\");\n",
    "    plt.legend();\n",
    "    plt.xlabel(\"x\");\n",
    "    plt.ylabel(\"y\");\n",
    "else:\n",
    "    plt.scatter(x_train[:,1], y_train, label=\"train data\");\n",
    "    plt.scatter(x_test[:,1], units[-1], label=\"test data prediction\");\n",
    "    plt.scatter(x_test[:,1], y_test, label=\"test data\");\n",
    "    plt.legend();\n",
    "    plt.xlabel(\"x\");\n",
    "    plt.ylabel(\"y\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise g) Show overfitting, underfitting and just right fitting\n",
    "\n",
    "Vary the architecture and other things to show clear signs of overfitting (=training loss significantly lower than test loss) and underfitting (=not fitting enoung to training data so that test performance is also hurt).\n",
    "\n",
    "See also if you can get a good compromise which leads to a low validation loss. \n",
    "\n",
    "For this problem do you see any big difference between validation and test loss? The answer here will probably be no. Discuss cases where it is important to keep the two separate.\n",
    "\n",
    "_Insert written answer here._\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Insert your code for getting overfitting, underfitting and just right fitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next steps - classification\n",
    "\n",
    "It is straight forward to extend what we have done to classification. \n",
    "\n",
    "For numerical stability it is better to make softmax and cross-entropy as one function so we write the cross entropy loss as a function of the logits we talked about last week. \n",
    "\n",
    "Next week we will see how to perform classification in PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise h) optional - Implement backpropagation for classification\n",
    "\n",
    "Should be possible with very few lines of code. :-)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Just add code."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
