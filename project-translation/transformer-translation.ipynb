{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tEnG_DGnkOVu"
   },
   "source": [
    "# https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OevlI7c7kOVw"
   },
   "outputs": [],
   "source": [
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 143
    },
    "colab_type": "code",
    "id": "znam-Cvul2_9",
    "outputId": "ce06f5cf-0bb6-4a32-a946-e5251b221c3f"
   },
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['names', 'eng-fra.txt']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "dataset_path = \"data/\"\n",
    "os.listdir(dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Yh0iAp2DkOVz"
   },
   "outputs": [],
   "source": [
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "\n",
    "\n",
    "class Language:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
    "        self.n_words = 2  # Count SOS and EOS\n",
    "\n",
    "    def add_sentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.add_word(word)\n",
    "\n",
    "    def add_word(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MpfbdnTfkOV2"
   },
   "outputs": [],
   "source": [
    "# Turn a Unicode string to plain ASCII, thanks to\n",
    "# https://stackoverflow.com/a/518232/2809427\n",
    "def unicode_to_ascii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "# Lowercase, trim, and remove non-letter characters\n",
    "def normalize_string(s):\n",
    "    s = unicode_to_ascii(s.lower().strip())\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jooSbq8ukOV5"
   },
   "outputs": [],
   "source": [
    "def read_languages(lang1, lang2, reverse=False):\n",
    "    # Read the file and split into lines\n",
    "    print(\"Reading lines...\")\n",
    "    lines = open(dataset_path + '%s-%s.txt' % (lang1, lang2), encoding='utf-8').\\\n",
    "        read().strip().split('\\n')\n",
    "\n",
    "    # Split every line into pairs and normalize\n",
    "    pairs = [[normalize_string(s) for s in l.split('\\t')] for l in lines]\n",
    "\n",
    "    # Reverse pairs\n",
    "    if reverse:\n",
    "        pairs = [list(reversed(p)) for p in pairs]\n",
    "    \n",
    "    ## Make Language objects\n",
    "    input_lang = Language(lang1)\n",
    "    output_lang = Language(lang2)\n",
    "\n",
    "    return input_lang, output_lang, pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "msj1xtHgkOV7"
   },
   "outputs": [],
   "source": [
    "MAX_LENGTH = 10\n",
    "\n",
    "eng_prefixes = (\n",
    "    \"i am \", \"i m \",\n",
    "    \"he is\", \"he s \",\n",
    "    \"she is\", \"she s \",\n",
    "    \"you are\", \"you re \",\n",
    "    \"we are\", \"we re \",\n",
    "    \"they are\", \"they re \"\n",
    ")\n",
    "\n",
    "\n",
    "def pair_predicate(p):\n",
    "    p1, p2 = p\n",
    "    return len(p1.split(' ')) < MAX_LENGTH and \\\n",
    "        len(p2.split(' ')) < MAX_LENGTH and \\\n",
    "        p2.startswith(eng_prefixes)\n",
    "\n",
    "def filter_pairs(pairs):\n",
    "    return [pair for pair in pairs if pair_predicate(pair)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 158
    },
    "colab_type": "code",
    "id": "_4XI-USJkOV9",
    "outputId": "5c0838e9-0e5b-404e-f529-ea94479af326"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines...\n",
      "Read 135842 sentence pairs\n",
      "Trimmed to 10599 sentence pairs\n",
      "Counting words...\n",
      "Counted words:\n",
      "eng 4345\n",
      "fra 2803\n",
      "['c est deja un homme .', 'he s already a man .']\n"
     ]
    }
   ],
   "source": [
    "def prepare_data(lang1, lang2, reverse=False):\n",
    "    input_lang, output_lang, pairs = read_languages(lang1, lang2, reverse)\n",
    "    \n",
    "    print(\"Read %s sentence pairs\" % len(pairs))\n",
    "    pairs = filter_pairs(pairs)\n",
    "    \n",
    "    print(\"Trimmed to %s sentence pairs\" % len(pairs))\n",
    "    print(\"Counting words...\")\n",
    "    \n",
    "    for pair in pairs:\n",
    "        input_lang.add_sentence(pair[0])\n",
    "        output_lang.add_sentence(pair[1])\n",
    "    print(\"Counted words:\")\n",
    "    print(input_lang.name, input_lang.n_words)\n",
    "    print(output_lang.name, output_lang.n_words)\n",
    "    return input_lang, output_lang, pairs\n",
    "\n",
    "\n",
    "input_lang, output_lang, pairs = prepare_data('eng', 'fra', True)\n",
    "print(random.choice(pairs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lYdptMWPnOlJ"
   },
   "source": [
    "## encoder network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "c3ucKOZVkOV_"
   },
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        x = self.embedding(input).view(1, 1, -1)\n",
    "        output, h = self.gru(x, hidden)\n",
    "        return output, h\n",
    "\n",
    "    def init_hidden_state(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9g4cJ3patvhw"
   },
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        x = self.embedding(input).view(1, 1, -1)\n",
    "        x = F.relu(x)\n",
    "        output, hidden = self.gru(x, hidden)\n",
    "\n",
    "        # Softmax over output to transform into word probability\n",
    "        output = self.softmax(self.out(output[0]))\n",
    "        return output, hidden\n",
    "\n",
    "    def init_hidden_state(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QTFhwDsSkOWD"
   },
   "outputs": [],
   "source": [
    "class AttentionDecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=MAX_LENGTH):\n",
    "        super(AttentionDecoderRNN, self).__init__()\n",
    "        \n",
    "        # Set dims\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.dropout_p = dropout_p\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        # Set layers\n",
    "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
    "        self.attention = nn.Linear(self.hidden_size * 2, self.max_length)\n",
    "        self.attention_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
    "        self.dropout = nn.Dropout(self.dropout_p)\n",
    "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
    "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
    "\n",
    "    def forward(self, input, hidden, encoder_outputs):\n",
    "        # Embed\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        embedded = self.dropout(embedded)\n",
    "\n",
    "        # Compute attention score as matrix product\n",
    "        attention_weights = F.softmax(self.attention(torch.cat((embedded[0], hidden[0]), 1)), dim=1)\n",
    "        attention_applied = torch.bmm(attention_weights.unsqueeze(0), encoder_outputs.unsqueeze(0))\n",
    "        \n",
    "        # Linear + RELU\n",
    "        x = torch.cat((embedded[0], attention_applied[0]), 1)\n",
    "        x = self.attention_combine(x).unsqueeze(0)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        # GRU RNN\n",
    "        output, hidden = self.gru(x, hidden)\n",
    "        \n",
    "        # Softmax over output to transform into word probability\n",
    "        output = F.log_softmax(self.out(output[0]), dim=1)\n",
    "        \n",
    "        return output, hidden, attention_weights\n",
    "\n",
    "    def init_hidden_state(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "\n",
    "    def __init__(self, ntoken, ninp, nhead, nhid, nlayers, dropout=0.5):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "        self.model_type = 'Transformer'\n",
    "        self.src_mask = None\n",
    "        self.pos_encoder = PositionalEncoding(ninp, dropout)\n",
    "        encoder_layers = TransformerEncoderLayer(ninp, nhead, nhid, dropout)\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n",
    "        self.encoder = nn.Embedding(ntoken, ninp)\n",
    "        self.ninp = ninp\n",
    "        self.decoder = nn.Linear(ninp, ntoken)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def _generate_square_subsequent_mask(self, sz):\n",
    "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "        return mask\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
    "        self.decoder.bias.data.zero_()\n",
    "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, src):\n",
    "        if self.src_mask is None or self.src_mask.size(0) != len(src):\n",
    "            device = src.device\n",
    "            mask = self._generate_square_subsequent_mask(len(src)).to(device)\n",
    "            self.src_mask = mask\n",
    "\n",
    "        src = self.encoder(src) * math.sqrt(self.ninp)\n",
    "        src = self.pos_encoder(src)\n",
    "        output = self.transformer_encoder(src, self.src_mask)\n",
    "        output = self.decoder(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Positional Encoding\n",
    "`PositionalEncoding` module injects some information about the relative or absolute position of the tokens in the sequence. The positional encodings have the same dimension as the embeddings so that the two can be summed. Here, we use sine and cosine functions of different frequencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AijG0yf7kOWF"
   },
   "outputs": [],
   "source": [
    "def sentence_to_index(lang, sentence):\n",
    "    return [lang.word2index[word] for word in sentence.split(' ')]\n",
    "\n",
    "\n",
    "def sentence_to_tensor(lang, sentence):\n",
    "    indexes = sentence_to_index(lang, sentence)\n",
    "    indexes.append(EOS_token)\n",
    "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n",
    "\n",
    "\n",
    "def pair_to_tensor(pair):\n",
    "    input_tensor = sentence_to_tensor(input_lang, pair[0])\n",
    "    target_tensor = sentence_to_tensor(output_lang, pair[1])\n",
    "    return (input_tensor, target_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Sn8a0ksskOWG"
   },
   "outputs": [],
   "source": [
    "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_LENGTH):\n",
    "    loss = 0\n",
    "\n",
    "    # Set optimizers\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    # Calculate sequence lengths\n",
    "    input_length = input_tensor.size(0)\n",
    "    target_length = target_tensor.size(0)\n",
    "    \n",
    "    # Initialize encoder output and hidden state as the zero vectors\n",
    "    encoder_hidden = encoder.init_hidden_state()\n",
    "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "    \n",
    "    # Unroll Encoder RNN\n",
    "    for t in range(input_length):\n",
    "        encoder_output, encoder_hidden = encoder(input_tensor[t], encoder_hidden)\n",
    "        encoder_outputs[t] = encoder_output[0, 0]\n",
    "\n",
    "    # After unrolling the Encoder RNN, the decoder takes the last encoder \n",
    "    # hidden state as it's initial hidden state\n",
    "    decoder_input = torch.tensor([[SOS_token]], device=device)\n",
    "    decoder_hidden = encoder_hidden\n",
    "\n",
    "    # Unroll Attention Decoder RNN, stop when most probable output token is the EOS token\n",
    "    for dt in range(target_length):\n",
    "        decoder_output, decoder_hidden, decoder_att = decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
    "        loss += criterion(decoder_output, target_tensor[dt])\n",
    "        \n",
    "        # Get index of the most probable token\n",
    "        _, argmax = decoder_output.topk(1)\n",
    "        decoder_input = argmax.squeeze().detach()  # detach from history as input\n",
    "        \n",
    "        # Stop unrolling if token is EOS\n",
    "        if decoder_input.item() == EOS_token:\n",
    "            break\n",
    "\n",
    "    # Perform BackProp\n",
    "    loss.backward()\n",
    "\n",
    "    # Tune params\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.item() / target_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "86EJbLUstZgK"
   },
   "source": [
    "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "\n",
    "    if use_teacher_forcing:\n",
    "        # Teacher forcing: Feed the target as the next input\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            decoder_input = target_tensor[di]  # Teacher forcing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9myKdlqykOWI"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "\n",
    "\n",
    "def as_minutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def time_since(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (as_minutes(s), as_minutes(rs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3yh8YlivkOWK"
   },
   "outputs": [],
   "source": [
    "def train_iterations(encoder, decoder, n_iters, print_every=1000, plot_every=100, learning_rate=0.01):\n",
    "    start = time.time()\n",
    "    plot_losses = []\n",
    "    print_loss_total = 0  # Reset every print_every\n",
    "    plot_loss_total = 0  # Reset every plot_every\n",
    "\n",
    "    # Init optimizers with Stochastic Gradient Descent\n",
    "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # Init loss function\n",
    "    criterion = nn.NLLLoss()\n",
    "\n",
    "    # Init dataset, pick random pairs\n",
    "    training_pairs = [pair_to_tensor(random.choice(pairs)) for i in range(n_iters)]\n",
    "    \n",
    "    for i in range(1, n_iters + 1):\n",
    "        # Split training pair\n",
    "        input_tensor, target_tensor = training_pairs[i - 1]\n",
    "\n",
    "        # Run through train algorithm\n",
    "        loss = train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "        \n",
    "        # Add losses to aux. variables\n",
    "        print_loss_total += loss\n",
    "        plot_loss_total += loss\n",
    "\n",
    "        if i % print_every == 0:\n",
    "            print_loss_avg = print_loss_total / print_every\n",
    "            print_loss_total = 0\n",
    "            print('%s (%d %d%%) %.4f' % (time_since(start, i / n_iters), i, i / n_iters * 100, print_loss_avg))\n",
    "\n",
    "        if i % plot_every == 0:\n",
    "            plot_loss_avg = plot_loss_total / plot_every\n",
    "            plot_losses.append(plot_loss_avg)\n",
    "            plot_loss_total = 0\n",
    "\n",
    "    show_loss_plot(plot_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9AO8q3h0kOWM"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.switch_backend('agg')\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def show_loss_plot(points):\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    # this locator puts ticks at regular intervals\n",
    "    loc = ticker.MultipleLocator(base=0.2)\n",
    "    ax.yaxis.set_major_locator(loc)\n",
    "    plt.plot(points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "W7SHOrihkOWR",
    "outputId": "26b317c3-38fa-4198-b176-bf573f6ea20d"
   },
   "outputs": [],
   "source": [
    "hidden_size = 256\n",
    "encoder1 = EncoderRNN(input_lang.n_words, hidden_size).to(device)\n",
    "decoder1 = AttentionDecoderRNN(hidden_size, output_lang.n_words, dropout_p=0.1).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0m 4s (- 60m 39s) (100 0%) 4.7307\n",
      "0m 8s (- 51m 7s) (200 0%) 3.6012\n",
      "0m 11s (- 48m 41s) (300 0%) 3.5900\n",
      "0m 15s (- 47m 17s) (400 0%) 3.4469\n",
      "0m 18s (- 46m 40s) (500 0%) 3.4119\n",
      "0m 22s (- 46m 15s) (600 0%) 3.4907\n",
      "0m 25s (- 45m 58s) (700 0%) 3.2503\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-a6b8f895dfe2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_iterations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m75000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_every\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-13-8b8ac4081591>\u001b[0m in \u001b[0;36mtrain_iterations\u001b[0;34m(encoder, decoder, n_iters, print_every, plot_every, learning_rate)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;31m# Run through train algorithm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_optimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_optimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;31m# Add losses to aux. variables\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-5ecb979f5d7c>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;31m# Perform BackProp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;31m# Tune params\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    148\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m         \"\"\"\n\u001b[0;32m--> 150\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_iterations(encoder1, decoder1, 75000, print_every=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wbakD4a1-MF3"
   },
   "source": [
    "Loss: 0.5736"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pG13VLVH-U4h"
   },
   "outputs": [],
   "source": [
    "def evaluate(encoder, decoder, sentence, max_length=MAX_LENGTH):\n",
    "    with torch.no_grad():\n",
    "        input_tensor = sentence_to_tensor(input_lang, sentence)\n",
    "        input_length = input_tensor.size()[0]\n",
    "        encoder_hidden = encoder.init_hidden_state()\n",
    "\n",
    "        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "\n",
    "        # Unroll Encoder RNN\n",
    "        for t in range(input_length):\n",
    "            encoder_output, encoder_hidden = encoder(input_tensor[t], encoder_hidden)\n",
    "            encoder_outputs[t] += encoder_output[0, 0]\n",
    "\n",
    "        # Initialize Decoder Hidden State as the last Encoder Hidden State\n",
    "        decoder_input = torch.tensor([[SOS_token]], device=device)  # SOS\n",
    "        decoder_hidden = encoder_hidden\n",
    "\n",
    "        decoded_words = []\n",
    "        decoder_attentions = torch.zeros(max_length, max_length)\n",
    "\n",
    "        # Unroll Decoder until most probable tokens is EOS\n",
    "        for dt in range(max_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
    "            decoder_attentions[dt] = decoder_attention.data\n",
    "\n",
    "            # Get index of most probable token (argmax)\n",
    "            _, argmax = decoder_output.data.topk(1)\n",
    "            \n",
    "            # Stop unrolling if EOS token is most probable\n",
    "            if argmax.item() == EOS_token:\n",
    "                decoded_words.append('<EOS>')\n",
    "                break\n",
    "            # If not EOS token, append the most probable word (convert index to string)\n",
    "            else:\n",
    "                decoded_words.append(output_lang.index2word[argmax.item()])\n",
    "            \n",
    "            # Stop keeping track of gradients..? i.e detach() function\n",
    "            decoder_input = argmax.squeeze().detach()\n",
    "\n",
    "        return decoded_words, decoder_attentions[:dt + 1]\n",
    "\n",
    "def evaluate_randomly(encoder, decoder, n=10):\n",
    "    for i in range(n):\n",
    "        source, target = random.choice(pairs)\n",
    "        print('SOURCE:', source)\n",
    "        print('TARGET:', target)\n",
    "        print('...')\n",
    "        # Translate source sentence\n",
    "        output_words, attentions = evaluate(encoder, decoder, source)\n",
    "        output_sentence = ' '.join(output_words)\n",
    "        print('PREDICTED:', output_sentence)\n",
    "        print('-'*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 905
    },
    "colab_type": "code",
    "id": "vab5Xx7akOWT",
    "outputId": "290d54fd-bb6f-49cc-aa10-aff80ce0f8e3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SOURCE: je suis plutot occupe .\n",
      "TARGET: i m rather busy .\n",
      "...\n",
      "PREDICTED: i m pretty busy . <EOS>\n",
      "--------------------------------------------------\n",
      "SOURCE: on est enfin seuls .\n",
      "TARGET: we re finally alone .\n",
      "...\n",
      "PREDICTED: we re finally alone . <EOS>\n",
      "--------------------------------------------------\n",
      "SOURCE: nous sommes tres occupees .\n",
      "TARGET: we re very busy .\n",
      "...\n",
      "PREDICTED: we re very busy . <EOS>\n",
      "--------------------------------------------------\n",
      "SOURCE: nous sommes en charge .\n",
      "TARGET: we re in charge .\n",
      "...\n",
      "PREDICTED: we re in charge . <EOS>\n",
      "--------------------------------------------------\n",
      "SOURCE: quelle commere tu fais .\n",
      "TARGET: you re such a tattletale .\n",
      "...\n",
      "PREDICTED: you are such the one . . <EOS>\n",
      "--------------------------------------------------\n",
      "SOURCE: je suis souvent en difficulte .\n",
      "TARGET: i am often in difficulties .\n",
      "...\n",
      "PREDICTED: i m in in trouble . <EOS>\n",
      "--------------------------------------------------\n",
      "SOURCE: nous sommes vieux amis .\n",
      "TARGET: we re old friends .\n",
      "...\n",
      "PREDICTED: we re old friends . <EOS>\n",
      "--------------------------------------------------\n",
      "SOURCE: j apprends aussi le francais .\n",
      "TARGET: i m also learning french .\n",
      "...\n",
      "PREDICTED: i am also learning french french . <EOS>\n",
      "--------------------------------------------------\n",
      "SOURCE: il souffre d amnesie .\n",
      "TARGET: he is suffering from loss of memory .\n",
      "...\n",
      "PREDICTED: he is suffering from from of . . <EOS>\n",
      "--------------------------------------------------\n",
      "SOURCE: nous sommes supposes connaitre les regles .\n",
      "TARGET: we are supposed to know the rules .\n",
      "...\n",
      "PREDICTED: we are supposed to know know rules . <EOS>\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "evaluate_randomly(encoder1, decoder1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load trained model instead\n",
    "encoder_dict = torch.load('saved-models/encoder-rnn.model', map_location=torch.device(device))\n",
    "decoder_dict = torch.load('saved-models/decoder-rnn-att.model', map_location=torch.device(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 256\n",
    "encoder1 = EncoderRNN(input_lang.n_words, hidden_size).to(device)\n",
    "decoder1 = AttentionDecoderRNN(hidden_size, output_lang.n_words, dropout_p=0.1).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder1.load_state_dict(encoder_dict)\n",
    "decoder1.load_state_dict(decoder_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SOURCE: ce n est qu une enfant .\n",
      "TARGET: she s just a child .\n",
      "...\n",
      "PREDICTED: she s just a child . <EOS>\n",
      "--------------------------------------------------\n",
      "SOURCE: elles ont de la chance d etre vivantes .\n",
      "TARGET: they re lucky to be alive .\n",
      "...\n",
      "PREDICTED: they re lucky to be alive . <EOS>\n",
      "--------------------------------------------------\n",
      "SOURCE: je suis fier de vous les gars .\n",
      "TARGET: i m proud of you guys .\n",
      "...\n",
      "PREDICTED: i m proud of you guys . <EOS>\n",
      "--------------------------------------------------\n",
      "SOURCE: ils sont dans la douche .\n",
      "TARGET: they re in the shower .\n",
      "...\n",
      "PREDICTED: they re in the shower . <EOS>\n",
      "--------------------------------------------------\n",
      "SOURCE: il est japonais de naissance .\n",
      "TARGET: he is japanese by birth .\n",
      "...\n",
      "PREDICTED: he is japanese in japanese . <EOS>\n",
      "--------------------------------------------------\n",
      "SOURCE: je ne suis pas libre cette apres midi .\n",
      "TARGET: i m not free to go this afternoon .\n",
      "...\n",
      "PREDICTED: i m not free to go free afternoon afternoon .\n",
      "--------------------------------------------------\n",
      "SOURCE: encore un petit effort .\n",
      "TARGET: we re almost there .\n",
      "...\n",
      "PREDICTED: we re almost there . <EOS>\n",
      "--------------------------------------------------\n",
      "SOURCE: elle est trop vieille pour toi .\n",
      "TARGET: she s too old for you .\n",
      "...\n",
      "PREDICTED: she s too old for you . <EOS>\n",
      "--------------------------------------------------\n",
      "SOURCE: nous sommes tous contents d aider .\n",
      "TARGET: we are all happy to help .\n",
      "...\n",
      "PREDICTED: we re all happy to help . <EOS>\n",
      "--------------------------------------------------\n",
      "SOURCE: tu rougis !\n",
      "TARGET: you re blushing .\n",
      "...\n",
      "PREDICTED: you re blushing . <EOS>\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "evaluate_randomly(encoder1, decoder1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "translation-rnn.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
