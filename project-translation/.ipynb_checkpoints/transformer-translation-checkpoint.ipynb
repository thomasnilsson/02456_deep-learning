{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tEnG_DGnkOVu"
   },
   "source": [
    "# https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OevlI7c7kOVw"
   },
   "outputs": [],
   "source": [
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 143
    },
    "colab_type": "code",
    "id": "znam-Cvul2_9",
    "outputId": "ce06f5cf-0bb6-4a32-a946-e5251b221c3f"
   },
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['names', '.DS_Store', 'eng-fra.txt', 'da-en']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "dataset_path = \"data/\"\n",
    "os.listdir(dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Yh0iAp2DkOVz"
   },
   "outputs": [],
   "source": [
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "\n",
    "\n",
    "class Language:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
    "        self.n_words = 2  # Count SOS and EOS\n",
    "\n",
    "    def add_sentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.add_word(word)\n",
    "\n",
    "    def add_word(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MpfbdnTfkOV2"
   },
   "outputs": [],
   "source": [
    "# Turn a Unicode string to plain ASCII, thanks to\n",
    "# https://stackoverflow.com/a/518232/2809427\n",
    "def unicode_to_ascii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "# Lowercase, trim, and remove non-letter characters\n",
    "def normalize_string(s):\n",
    "    s = unicode_to_ascii(s.lower().strip())\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jooSbq8ukOV5"
   },
   "outputs": [],
   "source": [
    "def read_languages(lang1, lang2, reverse=False):\n",
    "    # Read the file and split into lines\n",
    "    print(\"Reading lines...\")\n",
    "    lines = open(dataset_path + '%s-%s.txt' % (lang1, lang2), encoding='utf-8').\\\n",
    "        read().strip().split('\\n')\n",
    "\n",
    "    # Split every line into pairs and normalize\n",
    "    pairs = [[normalize_string(s) for s in l.split('\\t')] for l in lines]\n",
    "\n",
    "    # Reverse pairs\n",
    "    if reverse:\n",
    "        pairs = [list(reversed(p)) for p in pairs]\n",
    "    \n",
    "    ## Make Language objects\n",
    "    input_lang = Language(lang1)\n",
    "    output_lang = Language(lang2)\n",
    "\n",
    "    return input_lang, output_lang, pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "msj1xtHgkOV7"
   },
   "outputs": [],
   "source": [
    "MAX_LENGTH = 10\n",
    "\n",
    "eng_prefixes = (\n",
    "    \"i am \", \"i m \",\n",
    "    \"he is\", \"he s \",\n",
    "    \"she is\", \"she s \",\n",
    "    \"you are\", \"you re \",\n",
    "    \"we are\", \"we re \",\n",
    "    \"they are\", \"they re \"\n",
    ")\n",
    "\n",
    "\n",
    "def pair_predicate(p):\n",
    "    p1, p2 = p\n",
    "    return len(p1.split(' ')) < MAX_LENGTH and \\\n",
    "        len(p2.split(' ')) < MAX_LENGTH and \\\n",
    "        p2.startswith(eng_prefixes)\n",
    "\n",
    "def filter_pairs(pairs):\n",
    "    return [pair for pair in pairs if pair_predicate(pair)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 158
    },
    "colab_type": "code",
    "id": "_4XI-USJkOV9",
    "outputId": "5c0838e9-0e5b-404e-f529-ea94479af326"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines...\n",
      "Read 135842 sentence pairs\n",
      "Trimmed to 10599 sentence pairs\n",
      "Counting words...\n",
      "Counted words:\n",
      "eng 4345\n",
      "fra 2803\n",
      "['je ne suis pas comme toi .', 'i am not like you .']\n"
     ]
    }
   ],
   "source": [
    "def prepare_data(lang1, lang2, reverse=False):\n",
    "    input_lang, output_lang, pairs = read_languages(lang1, lang2, reverse)\n",
    "    \n",
    "    print(\"Read %s sentence pairs\" % len(pairs))\n",
    "    pairs = filter_pairs(pairs)\n",
    "    \n",
    "    print(\"Trimmed to %s sentence pairs\" % len(pairs))\n",
    "    print(\"Counting words...\")\n",
    "    \n",
    "    for pair in pairs:\n",
    "        input_lang.add_sentence(pair[0])\n",
    "        output_lang.add_sentence(pair[1])\n",
    "    print(\"Counted words:\")\n",
    "    print(input_lang.name, input_lang.n_words)\n",
    "    print(output_lang.name, output_lang.n_words)\n",
    "    return input_lang, output_lang, pairs\n",
    "\n",
    "\n",
    "input_lang, output_lang, pairs = prepare_data('eng', 'fra', True)\n",
    "print(random.choice(pairs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lYdptMWPnOlJ"
   },
   "source": [
    "## encoder network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "c3ucKOZVkOV_"
   },
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        x = self.embedding(input).view(1, 1, -1)\n",
    "        output, h = self.gru(x, hidden)\n",
    "        return output, h\n",
    "\n",
    "    def init_hidden_state(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9g4cJ3patvhw"
   },
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        x = self.embedding(input).view(1, 1, -1)\n",
    "        x = F.relu(x)\n",
    "        output, hidden = self.gru(x, hidden)\n",
    "\n",
    "        # Softmax over output to transform into word probability\n",
    "        output = self.softmax(self.out(output[0]))\n",
    "        return output, hidden\n",
    "\n",
    "    def init_hidden_state(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QTFhwDsSkOWD"
   },
   "outputs": [],
   "source": [
    "class AttentionDecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=MAX_LENGTH):\n",
    "        super(AttentionDecoderRNN, self).__init__()\n",
    "        \n",
    "        # Set dims\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.dropout_p = dropout_p\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        # Set layers\n",
    "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
    "        self.attention = nn.Linear(self.hidden_size * 2, self.max_length)\n",
    "        self.attention_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
    "        self.dropout = nn.Dropout(self.dropout_p)\n",
    "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
    "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
    "\n",
    "    def forward(self, input, hidden, encoder_outputs):\n",
    "        # Embed\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        embedded = self.dropout(embedded)\n",
    "\n",
    "        # Compute attention score as matrix product\n",
    "        attention_weights = F.softmax(self.attention(torch.cat((embedded[0], hidden[0]), 1)), dim=1)\n",
    "        attention_applied = torch.bmm(attention_weights.unsqueeze(0), encoder_outputs.unsqueeze(0))\n",
    "        \n",
    "        # Linear + RELU\n",
    "        x = torch.cat((embedded[0], attention_applied[0]), 1)\n",
    "        x = self.attention_combine(x).unsqueeze(0)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        # GRU RNN\n",
    "        output, hidden = self.gru(x, hidden)\n",
    "        \n",
    "        # Softmax over output to transform into word probability\n",
    "        output = F.log_softmax(self.out(output[0]), dim=1)\n",
    "        \n",
    "        return output, hidden, attention_weights\n",
    "\n",
    "    def init_hidden_state(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AijG0yf7kOWF"
   },
   "outputs": [],
   "source": [
    "def sentence_to_index(lang, sentence):\n",
    "    return [lang.word2index[word] for word in sentence.split(' ')]\n",
    "\n",
    "\n",
    "def sentence_to_tensor(lang, sentence):\n",
    "    indexes = sentence_to_index(lang, sentence)\n",
    "    indexes.append(EOS_token)\n",
    "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n",
    "\n",
    "\n",
    "def pair_to_tensor(pair):\n",
    "    input_tensor = sentence_to_tensor(input_lang, pair[0])\n",
    "    target_tensor = sentence_to_tensor(output_lang, pair[1])\n",
    "    return (input_tensor, target_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Sn8a0ksskOWG"
   },
   "outputs": [],
   "source": [
    "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_LENGTH):\n",
    "    loss = 0\n",
    "\n",
    "    # Set optimizers\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    # Calculate sequence lengths\n",
    "    input_length = input_tensor.size(0)\n",
    "    target_length = target_tensor.size(0)\n",
    "    \n",
    "    # Initialize encoder output and hidden state as the zero vectors\n",
    "    encoder_hidden = encoder.init_hidden_state()\n",
    "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "    \n",
    "    # Unroll Encoder RNN\n",
    "    for t in range(input_length):\n",
    "        encoder_output, encoder_hidden = encoder(input_tensor[t], encoder_hidden)\n",
    "        encoder_outputs[t] = encoder_output[0, 0]\n",
    "\n",
    "    # After unrolling the Encoder RNN, the decoder takes the last encoder \n",
    "    # hidden state as it's initial hidden state\n",
    "    decoder_input = torch.tensor([[SOS_token]], device=device)\n",
    "    decoder_hidden = encoder_hidden\n",
    "\n",
    "    # Unroll Attention Decoder RNN, stop when most probable output token is the EOS token\n",
    "    for dt in range(target_length):\n",
    "        decoder_output, decoder_hidden, decoder_att = decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
    "        loss += criterion(decoder_output, target_tensor[dt])\n",
    "        \n",
    "        # Get index of the most probable token\n",
    "        _, argmax = decoder_output.topk(1)\n",
    "        decoder_input = argmax.squeeze().detach()  # detach from history as input\n",
    "        \n",
    "        # Stop unrolling if token is EOS\n",
    "        if decoder_input.item() == EOS_token:\n",
    "            break\n",
    "\n",
    "    # Perform BackProp\n",
    "    loss.backward()\n",
    "\n",
    "    # Tune params\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.item() / target_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "86EJbLUstZgK"
   },
   "source": [
    "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "\n",
    "    if use_teacher_forcing:\n",
    "        # Teacher forcing: Feed the target as the next input\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            decoder_input = target_tensor[di]  # Teacher forcing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9myKdlqykOWI"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "\n",
    "\n",
    "def as_minutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def time_since(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (as_minutes(s), as_minutes(rs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3yh8YlivkOWK"
   },
   "outputs": [],
   "source": [
    "def train_iterations(encoder, decoder, n_iters, print_every=1000, plot_every=100, learning_rate=0.01):\n",
    "    start = time.time()\n",
    "    plot_losses = []\n",
    "    print_loss_total = 0  # Reset every print_every\n",
    "    plot_loss_total = 0  # Reset every plot_every\n",
    "\n",
    "    # Init optimizers with Stochastic Gradient Descent\n",
    "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # Init loss function\n",
    "    criterion = nn.NLLLoss()\n",
    "\n",
    "    # Init dataset, pick random pairs\n",
    "    training_pairs = [pair_to_tensor(random.choice(pairs)) for i in range(n_iters)]\n",
    "    \n",
    "    for i in range(1, n_iters + 1):\n",
    "        # Split training pair\n",
    "        input_tensor, target_tensor = training_pairs[i - 1]\n",
    "\n",
    "        # Run through train algorithm\n",
    "        loss = train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "        \n",
    "        # Add losses to aux. variables\n",
    "        print_loss_total += loss\n",
    "        plot_loss_total += loss\n",
    "\n",
    "        if i % print_every == 0:\n",
    "            print_loss_avg = print_loss_total / print_every\n",
    "            print_loss_total = 0\n",
    "            print('%s (%d %d%%) %.4f' % (time_since(start, i / n_iters), i, i / n_iters * 100, print_loss_avg))\n",
    "\n",
    "        if i % plot_every == 0:\n",
    "            plot_loss_avg = plot_loss_total / plot_every\n",
    "            plot_losses.append(plot_loss_avg)\n",
    "            plot_loss_total = 0\n",
    "\n",
    "    show_loss_plot(plot_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9AO8q3h0kOWM"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.switch_backend('agg')\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def show_loss_plot(points):\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    # this locator puts ticks at regular intervals\n",
    "    loc = ticker.MultipleLocator(base=0.2)\n",
    "    ax.yaxis.set_major_locator(loc)\n",
    "    plt.plot(points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uHTygV4kkOWO"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "V4vFG4itkOWQ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "W7SHOrihkOWR",
    "outputId": "26b317c3-38fa-4198-b176-bf573f6ea20d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0m 11s (- 143m 39s) (100 0%) 4.6575\n",
      "0m 13s (- 87m 13s) (200 0%) 3.5568\n",
      "0m 16s (- 68m 24s) (300 0%) 3.3727\n",
      "0m 19s (- 59m 31s) (400 0%) 3.4901\n",
      "0m 21s (- 54m 26s) (500 0%) 3.5336\n",
      "0m 24s (- 50m 37s) (600 0%) 3.2394\n",
      "0m 27s (- 48m 15s) (700 0%) 3.5104\n",
      "0m 30s (- 46m 22s) (800 1%) 3.3559\n",
      "0m 32s (- 45m 4s) (900 1%) 3.3527\n",
      "0m 35s (- 43m 56s) (1000 1%) 3.2686\n",
      "0m 38s (- 42m 53s) (1100 1%) 3.1503\n",
      "0m 41s (- 42m 3s) (1200 1%) 3.1440\n",
      "0m 43s (- 41m 21s) (1300 1%) 3.1097\n",
      "0m 46s (- 40m 47s) (1400 1%) 3.0481\n",
      "0m 49s (- 40m 10s) (1500 2%) 2.9967\n",
      "0m 51s (- 39m 38s) (1600 2%) 2.8555\n",
      "0m 54s (- 39m 12s) (1700 2%) 3.0507\n",
      "0m 57s (- 38m 40s) (1800 2%) 2.7788\n",
      "0m 59s (- 38m 23s) (1900 2%) 2.9779\n",
      "1m 2s (- 38m 4s) (2000 2%) 2.8141\n",
      "1m 5s (- 37m 43s) (2100 2%) 2.9102\n",
      "1m 7s (- 37m 28s) (2200 2%) 2.9186\n",
      "1m 10s (- 37m 13s) (2300 3%) 2.8519\n",
      "1m 13s (- 37m 5s) (2400 3%) 2.7987\n",
      "1m 16s (- 36m 55s) (2500 3%) 2.8806\n",
      "1m 19s (- 36m 42s) (2600 3%) 2.7920\n",
      "1m 22s (- 36m 35s) (2700 3%) 3.0036\n",
      "1m 24s (- 36m 27s) (2800 3%) 2.8829\n",
      "1m 27s (- 36m 16s) (2900 3%) 2.8468\n",
      "1m 30s (- 36m 6s) (3000 4%) 2.8262\n",
      "1m 32s (- 35m 52s) (3100 4%) 2.5452\n",
      "1m 35s (- 35m 43s) (3200 4%) 2.8890\n",
      "1m 38s (- 35m 34s) (3300 4%) 2.8243\n",
      "1m 40s (- 35m 26s) (3400 4%) 2.7229\n",
      "1m 43s (- 35m 16s) (3500 4%) 2.6797\n",
      "1m 46s (- 35m 6s) (3600 4%) 2.6748\n",
      "1m 48s (- 34m 57s) (3700 4%) 2.7059\n",
      "1m 51s (- 34m 55s) (3800 5%) 2.7209\n",
      "1m 54s (- 34m 53s) (3900 5%) 2.7201\n",
      "1m 57s (- 34m 51s) (4000 5%) 2.6161\n",
      "2m 0s (- 34m 47s) (4100 5%) 2.6261\n",
      "2m 3s (- 34m 44s) (4200 5%) 2.7065\n",
      "2m 6s (- 34m 40s) (4300 5%) 2.6269\n",
      "2m 9s (- 34m 35s) (4400 5%) 2.7329\n",
      "2m 12s (- 34m 29s) (4500 6%) 2.5721\n",
      "2m 14s (- 34m 23s) (4600 6%) 2.7719\n",
      "2m 17s (- 34m 19s) (4700 6%) 2.5876\n",
      "2m 20s (- 34m 15s) (4800 6%) 2.5606\n",
      "2m 23s (- 34m 8s) (4900 6%) 2.5738\n",
      "2m 25s (- 34m 3s) (5000 6%) 2.5571\n",
      "2m 28s (- 33m 58s) (5100 6%) 2.6981\n",
      "2m 31s (- 33m 53s) (5200 6%) 2.6429\n",
      "2m 34s (- 33m 46s) (5300 7%) 2.4914\n",
      "2m 36s (- 33m 39s) (5400 7%) 2.3630\n",
      "2m 39s (- 33m 32s) (5500 7%) 2.5267\n",
      "2m 41s (- 33m 27s) (5600 7%) 2.5421\n",
      "2m 44s (- 33m 21s) (5700 7%) 2.4593\n",
      "2m 47s (- 33m 16s) (5800 7%) 2.4726\n",
      "2m 49s (- 33m 10s) (5900 7%) 2.4642\n",
      "2m 52s (- 33m 6s) (6000 8%) 2.4748\n",
      "2m 55s (- 33m 0s) (6100 8%) 2.4940\n",
      "2m 58s (- 32m 55s) (6200 8%) 2.5634\n",
      "3m 0s (- 32m 52s) (6300 8%) 2.4750\n",
      "3m 3s (- 32m 47s) (6400 8%) 2.4710\n",
      "3m 6s (- 32m 40s) (6500 8%) 2.4067\n",
      "3m 8s (- 32m 36s) (6600 8%) 2.5243\n",
      "3m 11s (- 32m 32s) (6700 8%) 2.5039\n",
      "3m 14s (- 32m 28s) (6800 9%) 2.6049\n",
      "3m 17s (- 32m 25s) (6900 9%) 2.4196\n",
      "3m 19s (- 32m 21s) (7000 9%) 2.5107\n",
      "3m 22s (- 32m 17s) (7100 9%) 2.4265\n",
      "3m 25s (- 32m 14s) (7200 9%) 2.4833\n",
      "3m 28s (- 32m 10s) (7300 9%) 2.3376\n",
      "3m 30s (- 32m 7s) (7400 9%) 2.3980\n",
      "3m 33s (- 32m 4s) (7500 10%) 2.2546\n",
      "3m 36s (- 32m 2s) (7600 10%) 2.5389\n",
      "3m 39s (- 31m 59s) (7700 10%) 2.3730\n",
      "3m 42s (- 31m 57s) (7800 10%) 2.2965\n",
      "3m 45s (- 31m 55s) (7900 10%) 2.4939\n",
      "3m 48s (- 31m 51s) (8000 10%) 2.3767\n",
      "3m 51s (- 31m 48s) (8100 10%) 2.2193\n",
      "3m 53s (- 31m 45s) (8200 10%) 2.3831\n",
      "3m 56s (- 31m 42s) (8300 11%) 2.3554\n",
      "3m 59s (- 31m 39s) (8400 11%) 2.3717\n",
      "4m 2s (- 31m 36s) (8500 11%) 2.4421\n",
      "4m 5s (- 31m 33s) (8600 11%) 2.3374\n",
      "4m 7s (- 31m 29s) (8700 11%) 2.2659\n",
      "4m 10s (- 31m 26s) (8800 11%) 2.3533\n",
      "4m 13s (- 31m 23s) (8900 11%) 2.3719\n",
      "4m 16s (- 31m 20s) (9000 12%) 2.2560\n",
      "4m 19s (- 31m 18s) (9100 12%) 2.3787\n",
      "4m 22s (- 31m 16s) (9200 12%) 2.4047\n",
      "4m 25s (- 31m 12s) (9300 12%) 2.3478\n",
      "4m 27s (- 31m 8s) (9400 12%) 2.1281\n",
      "4m 30s (- 31m 4s) (9500 12%) 2.2129\n",
      "4m 33s (- 31m 0s) (9600 12%) 2.2701\n",
      "4m 35s (- 30m 57s) (9700 12%) 2.3678\n",
      "4m 38s (- 30m 53s) (9800 13%) 2.2308\n",
      "4m 41s (- 30m 49s) (9900 13%) 2.0221\n",
      "4m 43s (- 30m 45s) (10000 13%) 2.1869\n",
      "4m 46s (- 30m 43s) (10100 13%) 2.2533\n",
      "4m 49s (- 30m 41s) (10200 13%) 2.2689\n",
      "4m 52s (- 30m 39s) (10300 13%) 2.1180\n",
      "4m 55s (- 30m 37s) (10400 13%) 2.2543\n",
      "4m 58s (- 30m 35s) (10500 14%) 2.2410\n",
      "5m 1s (- 30m 31s) (10600 14%) 2.2275\n",
      "5m 4s (- 30m 27s) (10700 14%) 2.2379\n",
      "5m 6s (- 30m 24s) (10800 14%) 2.1499\n",
      "5m 9s (- 30m 21s) (10900 14%) 2.1642\n",
      "5m 12s (- 30m 18s) (11000 14%) 2.3140\n",
      "5m 15s (- 30m 14s) (11100 14%) 2.0478\n",
      "5m 17s (- 30m 11s) (11200 14%) 2.0887\n",
      "5m 20s (- 30m 8s) (11300 15%) 2.2862\n",
      "5m 23s (- 30m 4s) (11400 15%) 2.2633\n",
      "5m 26s (- 30m 1s) (11500 15%) 2.0968\n",
      "5m 28s (- 29m 57s) (11600 15%) 2.0741\n",
      "5m 31s (- 29m 53s) (11700 15%) 2.0966\n",
      "5m 34s (- 29m 50s) (11800 15%) 2.0977\n",
      "5m 36s (- 29m 46s) (11900 15%) 2.0266\n",
      "5m 39s (- 29m 43s) (12000 16%) 2.0415\n",
      "5m 42s (- 29m 39s) (12100 16%) 1.9246\n",
      "5m 45s (- 29m 36s) (12200 16%) 2.0723\n",
      "5m 47s (- 29m 32s) (12300 16%) 2.1121\n",
      "5m 50s (- 29m 29s) (12400 16%) 2.2437\n",
      "5m 53s (- 29m 26s) (12500 16%) 2.1909\n",
      "5m 56s (- 29m 24s) (12600 16%) 2.1573\n",
      "5m 59s (- 29m 21s) (12700 16%) 2.1131\n",
      "6m 1s (- 29m 18s) (12800 17%) 2.0679\n",
      "6m 4s (- 29m 16s) (12900 17%) 1.9075\n",
      "6m 7s (- 29m 13s) (13000 17%) 1.8637\n",
      "6m 10s (- 29m 10s) (13100 17%) 2.0015\n",
      "6m 13s (- 29m 7s) (13200 17%) 1.9545\n",
      "6m 16s (- 29m 4s) (13300 17%) 1.9989\n",
      "6m 18s (- 29m 1s) (13400 17%) 2.1388\n",
      "6m 21s (- 28m 57s) (13500 18%) 2.2051\n",
      "6m 24s (- 28m 54s) (13600 18%) 1.9225\n",
      "6m 27s (- 28m 51s) (13700 18%) 2.0624\n",
      "6m 29s (- 28m 48s) (13800 18%) 1.9192\n",
      "6m 32s (- 28m 45s) (13900 18%) 2.1368\n",
      "6m 35s (- 28m 43s) (14000 18%) 1.9039\n",
      "6m 38s (- 28m 40s) (14100 18%) 1.9854\n",
      "6m 41s (- 28m 37s) (14200 18%) 1.9972\n",
      "6m 44s (- 28m 34s) (14300 19%) 1.9264\n",
      "6m 46s (- 28m 32s) (14400 19%) 1.9619\n",
      "6m 49s (- 28m 29s) (14500 19%) 1.7618\n",
      "6m 52s (- 28m 26s) (14600 19%) 2.0036\n",
      "6m 55s (- 28m 23s) (14700 19%) 1.9093\n",
      "6m 58s (- 28m 20s) (14800 19%) 2.0231\n",
      "7m 0s (- 28m 17s) (14900 19%) 1.9458\n",
      "7m 3s (- 28m 14s) (15000 20%) 1.7964\n",
      "7m 6s (- 28m 12s) (15100 20%) 2.0452\n",
      "7m 9s (- 28m 10s) (15200 20%) 2.0624\n",
      "7m 12s (- 28m 6s) (15300 20%) 1.8147\n",
      "7m 14s (- 28m 3s) (15400 20%) 1.9355\n",
      "7m 17s (- 28m 0s) (15500 20%) 1.9415\n",
      "7m 20s (- 27m 56s) (15600 20%) 1.9202\n",
      "7m 23s (- 27m 53s) (15700 20%) 1.9457\n",
      "7m 25s (- 27m 50s) (15800 21%) 1.7973\n",
      "7m 28s (- 27m 46s) (15900 21%) 2.1192\n",
      "7m 31s (- 27m 43s) (16000 21%) 1.9726\n",
      "7m 34s (- 27m 41s) (16100 21%) 1.8198\n",
      "7m 37s (- 27m 38s) (16200 21%) 1.8607\n",
      "7m 40s (- 27m 36s) (16300 21%) 1.9165\n",
      "7m 43s (- 27m 34s) (16400 21%) 1.7944\n",
      "7m 46s (- 27m 33s) (16500 22%) 1.7362\n",
      "7m 49s (- 27m 30s) (16600 22%) 1.8665\n",
      "7m 51s (- 27m 27s) (16700 22%) 1.8103\n",
      "7m 54s (- 27m 24s) (16800 22%) 1.8797\n",
      "7m 57s (- 27m 22s) (16900 22%) 1.7781\n",
      "8m 0s (- 27m 19s) (17000 22%) 1.7332\n",
      "8m 3s (- 27m 16s) (17100 22%) 1.9493\n",
      "8m 5s (- 27m 13s) (17200 22%) 1.9368\n",
      "8m 8s (- 27m 9s) (17300 23%) 1.7555\n",
      "8m 11s (- 27m 6s) (17400 23%) 1.7652\n",
      "8m 14s (- 27m 3s) (17500 23%) 1.7789\n",
      "8m 16s (- 27m 0s) (17600 23%) 1.6207\n",
      "8m 19s (- 26m 57s) (17700 23%) 1.7357\n",
      "8m 22s (- 26m 54s) (17800 23%) 1.8641\n",
      "8m 25s (- 26m 51s) (17900 23%) 1.8452\n",
      "8m 28s (- 26m 48s) (18000 24%) 1.9728\n",
      "8m 30s (- 26m 45s) (18100 24%) 1.7165\n",
      "8m 33s (- 26m 43s) (18200 24%) 2.0289\n",
      "8m 36s (- 26m 40s) (18300 24%) 1.8037\n",
      "8m 39s (- 26m 37s) (18400 24%) 1.8693\n",
      "8m 42s (- 26m 35s) (18500 24%) 1.7285\n",
      "8m 45s (- 26m 32s) (18600 24%) 1.9782\n",
      "8m 48s (- 26m 30s) (18700 24%) 1.7996\n",
      "8m 50s (- 26m 27s) (18800 25%) 1.8875\n",
      "8m 53s (- 26m 24s) (18900 25%) 1.8041\n",
      "8m 56s (- 26m 21s) (19000 25%) 1.8687\n",
      "8m 59s (- 26m 19s) (19100 25%) 1.8273\n",
      "9m 2s (- 26m 16s) (19200 25%) 1.6808\n",
      "9m 4s (- 26m 12s) (19300 25%) 1.8051\n",
      "9m 7s (- 26m 9s) (19400 25%) 1.9674\n",
      "9m 10s (- 26m 7s) (19500 26%) 1.7316\n",
      "9m 13s (- 26m 4s) (19600 26%) 1.7446\n",
      "9m 16s (- 26m 2s) (19700 26%) 1.7034\n",
      "9m 19s (- 25m 59s) (19800 26%) 1.7452\n",
      "9m 22s (- 25m 57s) (19900 26%) 1.9515\n",
      "9m 25s (- 25m 54s) (20000 26%) 1.6907\n",
      "9m 28s (- 25m 51s) (20100 26%) 1.6639\n",
      "9m 30s (- 25m 48s) (20200 26%) 1.7003\n",
      "9m 33s (- 25m 45s) (20300 27%) 1.5753\n",
      "9m 36s (- 25m 42s) (20400 27%) 1.7240\n",
      "9m 39s (- 25m 40s) (20500 27%) 1.7985\n",
      "9m 42s (- 25m 38s) (20600 27%) 1.7298\n",
      "9m 45s (- 25m 35s) (20700 27%) 1.8328\n",
      "9m 48s (- 25m 33s) (20800 27%) 1.7320\n",
      "9m 51s (- 25m 30s) (20900 27%) 1.5838\n",
      "9m 54s (- 25m 27s) (21000 28%) 1.6864\n",
      "9m 56s (- 25m 24s) (21100 28%) 1.6409\n",
      "9m 59s (- 25m 21s) (21200 28%) 1.6822\n",
      "10m 2s (- 25m 18s) (21300 28%) 1.5276\n",
      "10m 5s (- 25m 15s) (21400 28%) 1.6055\n",
      "10m 7s (- 25m 12s) (21500 28%) 1.5743\n",
      "10m 10s (- 25m 9s) (21600 28%) 1.6836\n",
      "10m 13s (- 25m 6s) (21700 28%) 1.7309\n",
      "10m 16s (- 25m 3s) (21800 29%) 1.6244\n",
      "10m 18s (- 25m 0s) (21900 29%) 1.6652\n",
      "10m 21s (- 24m 57s) (22000 29%) 1.5805\n",
      "10m 24s (- 24m 54s) (22100 29%) 1.5057\n",
      "10m 27s (- 24m 52s) (22200 29%) 1.5878\n",
      "10m 30s (- 24m 49s) (22300 29%) 1.6632\n",
      "10m 33s (- 24m 46s) (22400 29%) 1.6417\n",
      "10m 35s (- 24m 43s) (22500 30%) 1.6154\n",
      "10m 38s (- 24m 40s) (22600 30%) 1.6782\n",
      "10m 41s (- 24m 38s) (22700 30%) 1.5217\n",
      "10m 44s (- 24m 35s) (22800 30%) 1.6745\n",
      "10m 47s (- 24m 33s) (22900 30%) 1.5795\n",
      "10m 50s (- 24m 30s) (23000 30%) 1.4574\n",
      "10m 53s (- 24m 27s) (23100 30%) 1.5648\n",
      "10m 56s (- 24m 24s) (23200 30%) 1.5547\n",
      "10m 59s (- 24m 22s) (23300 31%) 1.4975\n",
      "11m 1s (- 24m 19s) (23400 31%) 1.7753\n",
      "11m 4s (- 24m 16s) (23500 31%) 1.6407\n",
      "11m 7s (- 24m 13s) (23600 31%) 1.5234\n",
      "11m 10s (- 24m 10s) (23700 31%) 1.6926\n",
      "11m 13s (- 24m 8s) (23800 31%) 1.7013\n",
      "11m 15s (- 24m 5s) (23900 31%) 1.6672\n",
      "11m 18s (- 24m 2s) (24000 32%) 1.5261\n",
      "11m 21s (- 23m 59s) (24100 32%) 1.5082\n",
      "11m 24s (- 23m 57s) (24200 32%) 1.4653\n",
      "11m 27s (- 23m 54s) (24300 32%) 1.4278\n",
      "11m 30s (- 23m 51s) (24400 32%) 1.5874\n",
      "11m 33s (- 23m 48s) (24500 32%) 1.7545\n",
      "11m 35s (- 23m 45s) (24600 32%) 1.3957\n",
      "11m 38s (- 23m 42s) (24700 32%) 1.4814\n",
      "11m 41s (- 23m 39s) (24800 33%) 1.4296\n",
      "11m 43s (- 23m 36s) (24900 33%) 1.5476\n",
      "11m 46s (- 23m 33s) (25000 33%) 1.6767\n",
      "11m 49s (- 23m 31s) (25100 33%) 1.6585\n",
      "11m 52s (- 23m 28s) (25200 33%) 1.4307\n",
      "11m 55s (- 23m 26s) (25300 33%) 1.4600\n",
      "11m 58s (- 23m 23s) (25400 33%) 1.5125\n",
      "12m 1s (- 23m 20s) (25500 34%) 1.3566\n",
      "12m 4s (- 23m 18s) (25600 34%) 1.3970\n",
      "12m 7s (- 23m 15s) (25700 34%) 1.3961\n",
      "12m 9s (- 23m 12s) (25800 34%) 1.4443\n",
      "12m 12s (- 23m 9s) (25900 34%) 1.3704\n",
      "12m 15s (- 23m 6s) (26000 34%) 1.3994\n",
      "12m 18s (- 23m 3s) (26100 34%) 1.5223\n",
      "12m 20s (- 23m 0s) (26200 34%) 1.5554\n",
      "12m 23s (- 22m 57s) (26300 35%) 1.5270\n",
      "12m 26s (- 22m 54s) (26400 35%) 1.4652\n",
      "12m 29s (- 22m 52s) (26500 35%) 1.5257\n",
      "12m 32s (- 22m 49s) (26600 35%) 1.5336\n",
      "12m 35s (- 22m 46s) (26700 35%) 1.3225\n",
      "12m 38s (- 22m 43s) (26800 35%) 1.4348\n",
      "12m 40s (- 22m 40s) (26900 35%) 1.5564\n",
      "12m 43s (- 22m 38s) (27000 36%) 1.4353\n",
      "12m 46s (- 22m 35s) (27100 36%) 1.5973\n",
      "12m 49s (- 22m 32s) (27200 36%) 1.6773\n",
      "12m 52s (- 22m 29s) (27300 36%) 1.3223\n",
      "12m 55s (- 22m 27s) (27400 36%) 1.5234\n",
      "12m 58s (- 22m 24s) (27500 36%) 1.3570\n",
      "13m 1s (- 22m 21s) (27600 36%) 1.4560\n",
      "13m 3s (- 22m 18s) (27700 36%) 1.4765\n",
      "13m 6s (- 22m 15s) (27800 37%) 1.5353\n",
      "13m 9s (- 22m 12s) (27900 37%) 1.3815\n",
      "13m 12s (- 22m 9s) (28000 37%) 1.4852\n",
      "13m 15s (- 22m 7s) (28100 37%) 1.2857\n",
      "13m 17s (- 22m 4s) (28200 37%) 1.4763\n",
      "13m 20s (- 22m 1s) (28300 37%) 1.6646\n",
      "13m 23s (- 21m 58s) (28400 37%) 1.5366\n",
      "13m 26s (- 21m 55s) (28500 38%) 1.4284\n",
      "13m 29s (- 21m 53s) (28600 38%) 1.5983\n",
      "13m 32s (- 21m 50s) (28700 38%) 1.5393\n",
      "13m 35s (- 21m 47s) (28800 38%) 1.2733\n",
      "13m 38s (- 21m 44s) (28900 38%) 1.4649\n",
      "13m 40s (- 21m 42s) (29000 38%) 1.3255\n",
      "13m 43s (- 21m 39s) (29100 38%) 1.3630\n",
      "13m 46s (- 21m 36s) (29200 38%) 1.3700\n",
      "13m 49s (- 21m 33s) (29300 39%) 1.5275\n",
      "13m 52s (- 21m 31s) (29400 39%) 1.2447\n",
      "13m 55s (- 21m 28s) (29500 39%) 1.4100\n",
      "13m 57s (- 21m 25s) (29600 39%) 1.4372\n",
      "14m 0s (- 21m 22s) (29700 39%) 1.1548\n",
      "14m 3s (- 21m 18s) (29800 39%) 1.4298\n",
      "14m 5s (- 21m 15s) (29900 39%) 1.3914\n",
      "14m 8s (- 21m 13s) (30000 40%) 1.3538\n",
      "14m 11s (- 21m 10s) (30100 40%) 1.3252\n",
      "14m 14s (- 21m 7s) (30200 40%) 1.3222\n",
      "14m 17s (- 21m 4s) (30300 40%) 1.2317\n",
      "14m 20s (- 21m 2s) (30400 40%) 1.2350\n",
      "14m 23s (- 20m 59s) (30500 40%) 1.3518\n",
      "14m 26s (- 20m 57s) (30600 40%) 1.4617\n",
      "14m 29s (- 20m 54s) (30700 40%) 1.2665\n",
      "14m 32s (- 20m 51s) (30800 41%) 1.2914\n",
      "14m 35s (- 20m 49s) (30900 41%) 1.3599\n",
      "14m 38s (- 20m 46s) (31000 41%) 1.3201\n",
      "14m 41s (- 20m 43s) (31100 41%) 1.4060\n",
      "14m 43s (- 20m 40s) (31200 41%) 1.1672\n",
      "14m 46s (- 20m 37s) (31300 41%) 1.3955\n",
      "14m 49s (- 20m 34s) (31400 41%) 1.3223\n",
      "14m 52s (- 20m 32s) (31500 42%) 1.2709\n",
      "14m 55s (- 20m 29s) (31600 42%) 1.3844\n",
      "14m 57s (- 20m 26s) (31700 42%) 1.1836\n",
      "15m 0s (- 20m 23s) (31800 42%) 1.3301\n",
      "15m 3s (- 20m 20s) (31900 42%) 1.2067\n",
      "15m 5s (- 20m 17s) (32000 42%) 1.3896\n",
      "15m 8s (- 20m 14s) (32100 42%) 1.2357\n",
      "15m 11s (- 20m 11s) (32200 42%) 1.3333\n",
      "15m 14s (- 20m 8s) (32300 43%) 1.4629\n",
      "15m 17s (- 20m 5s) (32400 43%) 1.1635\n",
      "15m 20s (- 20m 3s) (32500 43%) 1.3071\n",
      "15m 22s (- 20m 0s) (32600 43%) 1.4200\n",
      "15m 25s (- 19m 57s) (32700 43%) 1.2927\n",
      "15m 28s (- 19m 54s) (32800 43%) 1.2366\n",
      "15m 31s (- 19m 52s) (32900 43%) 1.4881\n",
      "15m 34s (- 19m 49s) (33000 44%) 1.3048\n",
      "15m 37s (- 19m 46s) (33100 44%) 1.2232\n",
      "15m 40s (- 19m 43s) (33200 44%) 1.2847\n",
      "15m 42s (- 19m 40s) (33300 44%) 1.2840\n",
      "15m 45s (- 19m 37s) (33400 44%) 1.2763\n",
      "15m 48s (- 19m 35s) (33500 44%) 1.2296\n",
      "15m 51s (- 19m 32s) (33600 44%) 1.2785\n",
      "15m 54s (- 19m 29s) (33700 44%) 1.2219\n",
      "15m 57s (- 19m 26s) (33800 45%) 1.3807\n",
      "16m 0s (- 19m 23s) (33900 45%) 1.1916\n",
      "16m 3s (- 19m 21s) (34000 45%) 1.0989\n",
      "16m 5s (- 19m 18s) (34100 45%) 1.2850\n",
      "16m 8s (- 19m 15s) (34200 45%) 1.1766\n",
      "16m 11s (- 19m 12s) (34300 45%) 1.1321\n",
      "16m 14s (- 19m 9s) (34400 45%) 1.2974\n",
      "16m 17s (- 19m 7s) (34500 46%) 1.2399\n",
      "16m 19s (- 19m 4s) (34600 46%) 1.1336\n",
      "16m 22s (- 19m 1s) (34700 46%) 1.2805\n",
      "16m 25s (- 18m 58s) (34800 46%) 1.1289\n",
      "16m 28s (- 18m 55s) (34900 46%) 1.2061\n",
      "16m 31s (- 18m 53s) (35000 46%) 1.2932\n",
      "16m 34s (- 18m 50s) (35100 46%) 1.1565\n",
      "16m 37s (- 18m 48s) (35200 46%) 1.3273\n",
      "16m 40s (- 18m 45s) (35300 47%) 1.1648\n",
      "16m 43s (- 18m 42s) (35400 47%) 1.1259\n",
      "16m 46s (- 18m 39s) (35500 47%) 1.2942\n",
      "16m 49s (- 18m 36s) (35600 47%) 1.0744\n",
      "16m 52s (- 18m 34s) (35700 47%) 1.2768\n",
      "16m 54s (- 18m 31s) (35800 47%) 1.1072\n",
      "16m 57s (- 18m 28s) (35900 47%) 1.2631\n",
      "17m 0s (- 18m 25s) (36000 48%) 1.2741\n",
      "17m 3s (- 18m 22s) (36100 48%) 1.2719\n",
      "17m 5s (- 18m 19s) (36200 48%) 1.2342\n",
      "17m 8s (- 18m 16s) (36300 48%) 1.0768\n",
      "17m 11s (- 18m 13s) (36400 48%) 1.1347\n",
      "17m 14s (- 18m 10s) (36500 48%) 1.2814\n",
      "17m 16s (- 18m 7s) (36600 48%) 1.1898\n",
      "17m 19s (- 18m 5s) (36700 48%) 1.1936\n",
      "17m 22s (- 18m 2s) (36800 49%) 1.3424\n",
      "17m 25s (- 17m 59s) (36900 49%) 1.1163\n",
      "17m 28s (- 17m 56s) (37000 49%) 1.1391\n",
      "17m 30s (- 17m 53s) (37100 49%) 1.2262\n",
      "17m 33s (- 17m 50s) (37200 49%) 0.9861\n",
      "17m 36s (- 17m 47s) (37300 49%) 1.1912\n",
      "17m 39s (- 17m 45s) (37400 49%) 1.1539\n",
      "17m 42s (- 17m 42s) (37500 50%) 1.2302\n",
      "17m 45s (- 17m 39s) (37600 50%) 1.1871\n",
      "17m 48s (- 17m 36s) (37700 50%) 1.1078\n",
      "17m 51s (- 17m 34s) (37800 50%) 1.1080\n",
      "17m 53s (- 17m 31s) (37900 50%) 1.0769\n",
      "17m 56s (- 17m 28s) (38000 50%) 1.1733\n",
      "17m 59s (- 17m 25s) (38100 50%) 1.2527\n",
      "18m 2s (- 17m 23s) (38200 50%) 0.9854\n",
      "18m 5s (- 17m 20s) (38300 51%) 1.1433\n",
      "18m 8s (- 17m 17s) (38400 51%) 1.1153\n",
      "18m 11s (- 17m 14s) (38500 51%) 1.1639\n",
      "18m 13s (- 17m 11s) (38600 51%) 1.2157\n",
      "18m 16s (- 17m 8s) (38700 51%) 1.0573\n",
      "18m 19s (- 17m 5s) (38800 51%) 1.0864\n",
      "18m 22s (- 17m 2s) (38900 51%) 1.0947\n",
      "18m 24s (- 16m 59s) (39000 52%) 1.2800\n",
      "18m 27s (- 16m 57s) (39100 52%) 1.0585\n",
      "18m 30s (- 16m 54s) (39200 52%) 1.0590\n",
      "18m 33s (- 16m 51s) (39300 52%) 0.9286\n",
      "18m 36s (- 16m 48s) (39400 52%) 1.1943\n",
      "18m 39s (- 16m 45s) (39500 52%) 1.3124\n",
      "18m 42s (- 16m 43s) (39600 52%) 1.2271\n",
      "18m 44s (- 16m 40s) (39700 52%) 1.0833\n",
      "18m 47s (- 16m 37s) (39800 53%) 1.0585\n",
      "18m 50s (- 16m 34s) (39900 53%) 1.1741\n",
      "18m 53s (- 16m 31s) (40000 53%) 1.2191\n",
      "18m 56s (- 16m 28s) (40100 53%) 1.1496\n",
      "18m 58s (- 16m 25s) (40200 53%) 1.2069\n",
      "19m 1s (- 16m 22s) (40300 53%) 1.0752\n",
      "19m 4s (- 16m 20s) (40400 53%) 1.0923\n",
      "19m 7s (- 16m 17s) (40500 54%) 1.1605\n",
      "19m 10s (- 16m 14s) (40600 54%) 1.4226\n",
      "19m 13s (- 16m 11s) (40700 54%) 1.1217\n",
      "19m 16s (- 16m 9s) (40800 54%) 1.1435\n",
      "19m 19s (- 16m 6s) (40900 54%) 1.1634\n",
      "19m 22s (- 16m 3s) (41000 54%) 1.1645\n",
      "19m 25s (- 16m 1s) (41100 54%) 1.2063\n",
      "19m 28s (- 15m 58s) (41200 54%) 0.9455\n",
      "19m 30s (- 15m 55s) (41300 55%) 1.0748\n",
      "19m 33s (- 15m 52s) (41400 55%) 1.1452\n",
      "19m 36s (- 15m 49s) (41500 55%) 1.1491\n",
      "19m 39s (- 15m 47s) (41600 55%) 1.0638\n",
      "19m 42s (- 15m 44s) (41700 55%) 1.0224\n",
      "19m 45s (- 15m 41s) (41800 55%) 0.9190\n",
      "19m 47s (- 15m 38s) (41900 55%) 0.8662\n",
      "19m 50s (- 15m 35s) (42000 56%) 1.0230\n",
      "19m 53s (- 15m 32s) (42100 56%) 1.0639\n",
      "19m 56s (- 15m 29s) (42200 56%) 1.0059\n",
      "19m 59s (- 15m 27s) (42300 56%) 1.1103\n",
      "20m 2s (- 15m 24s) (42400 56%) 1.0843\n",
      "20m 5s (- 15m 21s) (42500 56%) 1.1416\n",
      "20m 8s (- 15m 18s) (42600 56%) 1.0507\n",
      "20m 10s (- 15m 16s) (42700 56%) 1.1182\n",
      "20m 13s (- 15m 13s) (42800 57%) 1.0653\n",
      "20m 16s (- 15m 10s) (42900 57%) 1.1807\n",
      "20m 19s (- 15m 7s) (43000 57%) 1.0924\n",
      "20m 22s (- 15m 4s) (43100 57%) 1.1330\n",
      "20m 25s (- 15m 1s) (43200 57%) 0.9798\n",
      "20m 28s (- 14m 59s) (43300 57%) 1.1443\n",
      "20m 30s (- 14m 56s) (43400 57%) 1.0128\n",
      "20m 33s (- 14m 53s) (43500 57%) 1.0850\n",
      "20m 36s (- 14m 50s) (43600 58%) 0.9573\n",
      "20m 39s (- 14m 47s) (43700 58%) 1.1146\n",
      "20m 42s (- 14m 45s) (43800 58%) 0.9502\n",
      "20m 45s (- 14m 42s) (43900 58%) 0.9268\n",
      "20m 48s (- 14m 39s) (44000 58%) 0.9530\n",
      "20m 50s (- 14m 36s) (44100 58%) 0.9882\n",
      "20m 54s (- 14m 33s) (44200 58%) 1.0729\n",
      "20m 57s (- 14m 31s) (44300 59%) 1.0046\n",
      "21m 0s (- 14m 28s) (44400 59%) 1.1721\n",
      "21m 3s (- 14m 25s) (44500 59%) 0.9666\n",
      "21m 5s (- 14m 22s) (44600 59%) 0.9384\n",
      "21m 8s (- 14m 19s) (44700 59%) 1.0475\n",
      "21m 11s (- 14m 17s) (44800 59%) 1.0262\n",
      "21m 14s (- 14m 14s) (44900 59%) 0.9251\n",
      "21m 16s (- 14m 11s) (45000 60%) 0.8886\n",
      "21m 19s (- 14m 8s) (45100 60%) 0.9349\n",
      "21m 22s (- 14m 5s) (45200 60%) 1.0075\n",
      "21m 25s (- 14m 2s) (45300 60%) 0.8495\n",
      "21m 28s (- 13m 59s) (45400 60%) 0.9430\n",
      "21m 30s (- 13m 56s) (45500 60%) 1.0146\n",
      "21m 33s (- 13m 54s) (45600 60%) 0.9496\n",
      "21m 36s (- 13m 51s) (45700 60%) 0.9898\n",
      "21m 39s (- 13m 48s) (45800 61%) 0.9955\n",
      "21m 42s (- 13m 45s) (45900 61%) 1.0542\n",
      "21m 44s (- 13m 42s) (46000 61%) 0.9851\n",
      "21m 47s (- 13m 39s) (46100 61%) 1.0072\n",
      "21m 50s (- 13m 36s) (46200 61%) 0.9256\n",
      "21m 53s (- 13m 34s) (46300 61%) 1.0056\n",
      "21m 56s (- 13m 31s) (46400 61%) 0.9113\n",
      "21m 58s (- 13m 28s) (46500 62%) 0.9838\n",
      "22m 1s (- 13m 25s) (46600 62%) 0.9236\n",
      "22m 4s (- 13m 22s) (46700 62%) 0.7924\n",
      "22m 7s (- 13m 19s) (46800 62%) 0.9560\n",
      "22m 10s (- 13m 17s) (46900 62%) 1.0210\n",
      "22m 13s (- 13m 14s) (47000 62%) 0.8281\n",
      "22m 16s (- 13m 11s) (47100 62%) 1.0394\n",
      "22m 19s (- 13m 8s) (47200 62%) 1.1676\n",
      "22m 21s (- 13m 5s) (47300 63%) 1.0945\n",
      "22m 24s (- 13m 3s) (47400 63%) 0.9740\n",
      "22m 27s (- 13m 0s) (47500 63%) 1.0083\n",
      "22m 30s (- 12m 57s) (47600 63%) 1.0756\n",
      "22m 33s (- 12m 54s) (47700 63%) 1.0702\n",
      "22m 36s (- 12m 51s) (47800 63%) 0.8652\n",
      "22m 39s (- 12m 49s) (47900 63%) 0.9221\n",
      "22m 42s (- 12m 46s) (48000 64%) 1.0043\n",
      "22m 44s (- 12m 43s) (48100 64%) 0.8652\n",
      "22m 47s (- 12m 40s) (48200 64%) 1.1486\n",
      "22m 50s (- 12m 37s) (48300 64%) 0.8684\n",
      "22m 53s (- 12m 34s) (48400 64%) 0.9372\n",
      "22m 56s (- 12m 32s) (48500 64%) 1.1744\n",
      "22m 59s (- 12m 29s) (48600 64%) 1.0764\n",
      "23m 2s (- 12m 26s) (48700 64%) 0.9120\n",
      "23m 4s (- 12m 23s) (48800 65%) 1.0122\n",
      "23m 7s (- 12m 20s) (48900 65%) 0.9779\n",
      "23m 10s (- 12m 17s) (49000 65%) 0.9894\n",
      "23m 13s (- 12m 14s) (49100 65%) 0.8789\n",
      "23m 15s (- 12m 11s) (49200 65%) 1.0793\n",
      "23m 18s (- 12m 9s) (49300 65%) 0.9587\n",
      "23m 21s (- 12m 6s) (49400 65%) 1.0415\n",
      "23m 24s (- 12m 3s) (49500 66%) 1.1155\n",
      "23m 27s (- 12m 0s) (49600 66%) 0.6808\n",
      "23m 29s (- 11m 57s) (49700 66%) 1.0617\n",
      "23m 32s (- 11m 54s) (49800 66%) 0.9479\n",
      "23m 35s (- 11m 52s) (49900 66%) 0.8277\n",
      "23m 38s (- 11m 49s) (50000 66%) 1.0327\n",
      "23m 41s (- 11m 46s) (50100 66%) 0.7996\n",
      "23m 44s (- 11m 43s) (50200 66%) 1.0242\n",
      "23m 47s (- 11m 40s) (50300 67%) 0.8984\n",
      "23m 50s (- 11m 38s) (50400 67%) 0.9161\n",
      "23m 53s (- 11m 35s) (50500 67%) 1.0725\n",
      "23m 55s (- 11m 32s) (50600 67%) 0.9805\n",
      "23m 58s (- 11m 29s) (50700 67%) 0.8645\n",
      "24m 1s (- 11m 26s) (50800 67%) 0.9841\n",
      "24m 4s (- 11m 23s) (50900 67%) 0.8269\n",
      "24m 6s (- 11m 20s) (51000 68%) 0.9432\n",
      "24m 9s (- 11m 17s) (51100 68%) 0.8815\n",
      "24m 12s (- 11m 15s) (51200 68%) 0.9827\n",
      "24m 14s (- 11m 12s) (51300 68%) 1.0110\n",
      "24m 18s (- 11m 9s) (51400 68%) 0.9461\n",
      "24m 21s (- 11m 6s) (51500 68%) 0.9683\n",
      "24m 24s (- 11m 3s) (51600 68%) 0.7231\n",
      "24m 27s (- 11m 1s) (51700 68%) 0.8463\n",
      "24m 30s (- 10m 58s) (51800 69%) 0.8893\n",
      "24m 33s (- 10m 55s) (51900 69%) 0.8202\n",
      "24m 35s (- 10m 52s) (52000 69%) 0.9765\n",
      "24m 38s (- 10m 50s) (52100 69%) 0.9583\n",
      "24m 41s (- 10m 47s) (52200 69%) 0.9329\n",
      "24m 44s (- 10m 44s) (52300 69%) 0.9601\n",
      "24m 47s (- 10m 41s) (52400 69%) 0.9677\n",
      "24m 50s (- 10m 38s) (52500 70%) 0.8610\n",
      "24m 52s (- 10m 35s) (52600 70%) 0.7426\n",
      "24m 55s (- 10m 32s) (52700 70%) 0.9420\n",
      "24m 58s (- 10m 29s) (52800 70%) 0.7325\n",
      "25m 1s (- 10m 27s) (52900 70%) 0.8868\n",
      "25m 4s (- 10m 24s) (53000 70%) 0.9936\n",
      "25m 6s (- 10m 21s) (53100 70%) 0.9323\n",
      "25m 9s (- 10m 18s) (53200 70%) 0.7671\n",
      "25m 12s (- 10m 15s) (53300 71%) 1.0282\n",
      "25m 15s (- 10m 12s) (53400 71%) 0.7841\n",
      "25m 18s (- 10m 10s) (53500 71%) 0.6634\n",
      "25m 20s (- 10m 7s) (53600 71%) 0.8013\n",
      "25m 23s (- 10m 4s) (53700 71%) 0.7653\n",
      "25m 26s (- 10m 1s) (53800 71%) 0.7602\n",
      "25m 29s (- 9m 58s) (53900 71%) 0.8371\n",
      "25m 32s (- 9m 55s) (54000 72%) 1.0502\n",
      "25m 35s (- 9m 53s) (54100 72%) 0.9216\n",
      "25m 38s (- 9m 50s) (54200 72%) 0.8033\n",
      "25m 41s (- 9m 47s) (54300 72%) 0.7664\n",
      "25m 44s (- 9m 44s) (54400 72%) 0.8383\n",
      "25m 47s (- 9m 41s) (54500 72%) 0.8080\n",
      "25m 50s (- 9m 39s) (54600 72%) 0.7730\n",
      "25m 52s (- 9m 36s) (54700 72%) 0.8137\n",
      "25m 55s (- 9m 33s) (54800 73%) 0.6457\n",
      "25m 58s (- 9m 30s) (54900 73%) 0.8301\n",
      "26m 1s (- 9m 27s) (55000 73%) 0.8014\n",
      "26m 4s (- 9m 25s) (55100 73%) 0.7994\n",
      "26m 7s (- 9m 22s) (55200 73%) 0.8195\n",
      "26m 10s (- 9m 19s) (55300 73%) 0.8555\n",
      "26m 13s (- 9m 16s) (55400 73%) 0.8628\n",
      "26m 16s (- 9m 13s) (55500 74%) 0.8206\n",
      "26m 18s (- 9m 10s) (55600 74%) 0.8765\n",
      "26m 21s (- 9m 8s) (55700 74%) 0.9235\n",
      "26m 24s (- 9m 5s) (55800 74%) 1.0145\n",
      "26m 27s (- 9m 2s) (55900 74%) 0.7714\n",
      "26m 30s (- 8m 59s) (56000 74%) 0.8665\n",
      "26m 32s (- 8m 56s) (56100 74%) 0.7872\n",
      "26m 35s (- 8m 53s) (56200 74%) 0.9863\n",
      "26m 38s (- 8m 50s) (56300 75%) 0.8769\n",
      "26m 41s (- 8m 48s) (56400 75%) 0.7897\n",
      "26m 44s (- 8m 45s) (56500 75%) 0.8097\n",
      "26m 46s (- 8m 42s) (56600 75%) 0.7919\n",
      "26m 49s (- 8m 39s) (56700 75%) 0.8961\n",
      "26m 52s (- 8m 36s) (56800 75%) 1.1652\n",
      "26m 55s (- 8m 33s) (56900 75%) 0.8701\n",
      "26m 58s (- 8m 31s) (57000 76%) 0.8701\n",
      "27m 1s (- 8m 28s) (57100 76%) 0.6955\n",
      "27m 4s (- 8m 25s) (57200 76%) 0.9415\n",
      "27m 7s (- 8m 22s) (57300 76%) 0.9128\n",
      "27m 10s (- 8m 19s) (57400 76%) 0.8729\n",
      "27m 12s (- 8m 16s) (57500 76%) 0.7872\n",
      "27m 15s (- 8m 14s) (57600 76%) 0.7288\n",
      "27m 18s (- 8m 11s) (57700 76%) 0.6964\n",
      "27m 21s (- 8m 8s) (57800 77%) 0.8259\n",
      "27m 24s (- 8m 5s) (57900 77%) 0.8610\n",
      "27m 27s (- 8m 2s) (58000 77%) 0.7778\n",
      "27m 29s (- 7m 59s) (58100 77%) 0.7952\n",
      "27m 32s (- 7m 57s) (58200 77%) 0.7141\n",
      "27m 35s (- 7m 54s) (58300 77%) 0.8937\n",
      "27m 38s (- 7m 51s) (58400 77%) 0.8628\n",
      "27m 41s (- 7m 48s) (58500 78%) 0.7799\n",
      "27m 44s (- 7m 45s) (58600 78%) 0.6762\n",
      "27m 47s (- 7m 43s) (58700 78%) 0.7224\n",
      "27m 50s (- 7m 40s) (58800 78%) 0.7720\n",
      "27m 53s (- 7m 37s) (58900 78%) 0.8711\n",
      "27m 56s (- 7m 34s) (59000 78%) 0.7792\n",
      "27m 59s (- 7m 31s) (59100 78%) 0.8271\n",
      "28m 2s (- 7m 29s) (59200 78%) 0.8114\n",
      "28m 5s (- 7m 26s) (59300 79%) 0.6676\n",
      "28m 8s (- 7m 23s) (59400 79%) 0.8084\n",
      "28m 10s (- 7m 20s) (59500 79%) 0.7708\n",
      "28m 13s (- 7m 17s) (59600 79%) 0.8391\n",
      "28m 16s (- 7m 14s) (59700 79%) 0.9519\n",
      "28m 19s (- 7m 11s) (59800 79%) 0.7630\n",
      "28m 22s (- 7m 9s) (59900 79%) 0.7874\n",
      "28m 25s (- 7m 6s) (60000 80%) 0.7551\n",
      "28m 28s (- 7m 3s) (60100 80%) 0.7929\n",
      "28m 31s (- 7m 0s) (60200 80%) 0.7039\n",
      "28m 33s (- 6m 57s) (60300 80%) 0.8111\n",
      "28m 36s (- 6m 54s) (60400 80%) 0.8427\n",
      "28m 39s (- 6m 52s) (60500 80%) 0.6324\n",
      "28m 42s (- 6m 49s) (60600 80%) 0.7599\n",
      "28m 45s (- 6m 46s) (60700 80%) 0.6374\n",
      "28m 48s (- 6m 43s) (60800 81%) 0.8767\n",
      "28m 51s (- 6m 40s) (60900 81%) 0.7569\n",
      "28m 54s (- 6m 38s) (61000 81%) 0.7195\n",
      "28m 57s (- 6m 35s) (61100 81%) 0.7369\n",
      "29m 0s (- 6m 32s) (61200 81%) 0.8292\n",
      "29m 2s (- 6m 29s) (61300 81%) 0.9003\n",
      "29m 5s (- 6m 26s) (61400 81%) 0.8457\n",
      "29m 8s (- 6m 23s) (61500 82%) 0.7959\n",
      "29m 11s (- 6m 20s) (61600 82%) 0.5870\n",
      "29m 14s (- 6m 18s) (61700 82%) 0.6133\n",
      "29m 17s (- 6m 15s) (61800 82%) 0.6649\n",
      "29m 20s (- 6m 12s) (61900 82%) 0.6521\n",
      "29m 23s (- 6m 9s) (62000 82%) 0.8103\n",
      "29m 26s (- 6m 6s) (62100 82%) 0.6426\n",
      "29m 29s (- 6m 4s) (62200 82%) 0.8119\n",
      "29m 32s (- 6m 1s) (62300 83%) 0.7850\n",
      "29m 35s (- 5m 58s) (62400 83%) 0.6728\n",
      "29m 38s (- 5m 55s) (62500 83%) 0.6946\n",
      "29m 40s (- 5m 52s) (62600 83%) 0.7157\n",
      "29m 43s (- 5m 49s) (62700 83%) 0.7703\n",
      "29m 46s (- 5m 47s) (62800 83%) 0.7191\n",
      "29m 49s (- 5m 44s) (62900 83%) 0.6991\n",
      "29m 51s (- 5m 41s) (63000 84%) 0.7273\n",
      "29m 54s (- 5m 38s) (63100 84%) 0.5911\n",
      "29m 57s (- 5m 35s) (63200 84%) 0.7200\n",
      "30m 0s (- 5m 32s) (63300 84%) 0.6383\n",
      "30m 3s (- 5m 29s) (63400 84%) 0.7873\n",
      "30m 6s (- 5m 27s) (63500 84%) 0.7724\n",
      "30m 8s (- 5m 24s) (63600 84%) 0.6184\n",
      "30m 11s (- 5m 21s) (63700 84%) 0.5732\n",
      "30m 14s (- 5m 18s) (63800 85%) 0.6623\n",
      "30m 17s (- 5m 15s) (63900 85%) 0.6924\n",
      "30m 20s (- 5m 12s) (64000 85%) 0.7614\n",
      "30m 23s (- 5m 10s) (64100 85%) 0.8331\n",
      "30m 26s (- 5m 7s) (64200 85%) 0.6889\n",
      "30m 29s (- 5m 4s) (64300 85%) 0.6739\n",
      "30m 32s (- 5m 1s) (64400 85%) 0.7506\n",
      "30m 35s (- 4m 58s) (64500 86%) 0.5773\n",
      "30m 37s (- 4m 55s) (64600 86%) 0.6802\n",
      "30m 40s (- 4m 53s) (64700 86%) 0.8253\n",
      "30m 43s (- 4m 50s) (64800 86%) 0.6870\n",
      "30m 46s (- 4m 47s) (64900 86%) 0.6553\n",
      "30m 49s (- 4m 44s) (65000 86%) 0.6357\n",
      "30m 52s (- 4m 41s) (65100 86%) 0.7980\n",
      "30m 55s (- 4m 38s) (65200 86%) 0.7625\n",
      "30m 58s (- 4m 36s) (65300 87%) 0.8292\n",
      "31m 1s (- 4m 33s) (65400 87%) 0.7145\n",
      "31m 3s (- 4m 30s) (65500 87%) 0.6461\n",
      "31m 6s (- 4m 27s) (65600 87%) 0.6250\n",
      "31m 9s (- 4m 24s) (65700 87%) 0.5650\n",
      "31m 12s (- 4m 21s) (65800 87%) 0.8237\n",
      "31m 15s (- 4m 18s) (65900 87%) 0.6842\n",
      "31m 18s (- 4m 16s) (66000 88%) 0.7085\n",
      "31m 21s (- 4m 13s) (66100 88%) 0.5668\n",
      "31m 23s (- 4m 10s) (66200 88%) 0.6427\n",
      "31m 26s (- 4m 7s) (66300 88%) 0.6869\n",
      "31m 29s (- 4m 4s) (66400 88%) 0.5993\n",
      "31m 32s (- 4m 1s) (66500 88%) 0.7595\n",
      "31m 35s (- 3m 59s) (66600 88%) 0.6939\n",
      "31m 38s (- 3m 56s) (66700 88%) 0.4768\n",
      "31m 40s (- 3m 53s) (66800 89%) 0.5681\n",
      "31m 43s (- 3m 50s) (66900 89%) 0.6270\n",
      "31m 46s (- 3m 47s) (67000 89%) 0.8272\n",
      "31m 49s (- 3m 44s) (67100 89%) 0.5958\n",
      "31m 52s (- 3m 42s) (67200 89%) 0.5428\n",
      "31m 55s (- 3m 39s) (67300 89%) 0.7447\n",
      "31m 58s (- 3m 36s) (67400 89%) 0.6894\n",
      "32m 1s (- 3m 33s) (67500 90%) 0.6786\n",
      "32m 3s (- 3m 30s) (67600 90%) 0.6617\n",
      "32m 6s (- 3m 27s) (67700 90%) 0.6897\n",
      "32m 9s (- 3m 24s) (67800 90%) 0.6579\n",
      "32m 12s (- 3m 22s) (67900 90%) 0.6927\n",
      "32m 15s (- 3m 19s) (68000 90%) 0.8066\n",
      "32m 18s (- 3m 16s) (68100 90%) 0.6630\n",
      "32m 21s (- 3m 13s) (68200 90%) 0.7749\n",
      "32m 24s (- 3m 10s) (68300 91%) 0.6417\n",
      "32m 27s (- 3m 7s) (68400 91%) 0.6132\n",
      "32m 30s (- 3m 5s) (68500 91%) 0.6526\n",
      "32m 33s (- 3m 2s) (68600 91%) 0.6215\n",
      "32m 36s (- 2m 59s) (68700 91%) 0.6741\n",
      "32m 39s (- 2m 56s) (68800 91%) 0.6117\n",
      "32m 42s (- 2m 53s) (68900 91%) 0.5284\n",
      "32m 45s (- 2m 50s) (69000 92%) 0.5983\n",
      "32m 48s (- 2m 48s) (69100 92%) 0.6083\n",
      "32m 50s (- 2m 45s) (69200 92%) 0.6881\n",
      "32m 53s (- 2m 42s) (69300 92%) 0.5300\n",
      "32m 56s (- 2m 39s) (69400 92%) 0.6043\n",
      "32m 59s (- 2m 36s) (69500 92%) 0.6990\n",
      "33m 2s (- 2m 33s) (69600 92%) 0.6428\n",
      "33m 4s (- 2m 30s) (69700 92%) 0.5727\n",
      "33m 7s (- 2m 28s) (69800 93%) 0.6393\n",
      "33m 10s (- 2m 25s) (69900 93%) 0.6682\n",
      "33m 13s (- 2m 22s) (70000 93%) 0.6186\n",
      "33m 16s (- 2m 19s) (70100 93%) 0.5164\n",
      "33m 19s (- 2m 16s) (70200 93%) 0.5383\n",
      "33m 22s (- 2m 13s) (70300 93%) 0.5881\n",
      "33m 24s (- 2m 11s) (70400 93%) 0.6670\n",
      "33m 27s (- 2m 8s) (70500 94%) 0.6085\n",
      "33m 30s (- 2m 5s) (70600 94%) 0.5765\n",
      "33m 33s (- 2m 2s) (70700 94%) 0.5516\n",
      "33m 36s (- 1m 59s) (70800 94%) 0.7314\n",
      "33m 39s (- 1m 56s) (70900 94%) 0.7449\n",
      "33m 42s (- 1m 53s) (71000 94%) 0.6421\n",
      "33m 45s (- 1m 51s) (71100 94%) 0.5434\n",
      "33m 48s (- 1m 48s) (71200 94%) 0.6426\n",
      "33m 51s (- 1m 45s) (71300 95%) 0.5940\n",
      "33m 53s (- 1m 42s) (71400 95%) 0.5573\n",
      "33m 56s (- 1m 39s) (71500 95%) 0.5438\n",
      "33m 59s (- 1m 36s) (71600 95%) 0.7062\n",
      "34m 2s (- 1m 34s) (71700 95%) 0.5265\n",
      "34m 5s (- 1m 31s) (71800 95%) 0.5543\n",
      "34m 8s (- 1m 28s) (71900 95%) 0.6381\n",
      "34m 11s (- 1m 25s) (72000 96%) 0.6652\n",
      "34m 13s (- 1m 22s) (72100 96%) 0.5992\n",
      "34m 16s (- 1m 19s) (72200 96%) 0.6062\n",
      "34m 19s (- 1m 16s) (72300 96%) 0.6061\n",
      "34m 22s (- 1m 14s) (72400 96%) 0.6762\n",
      "34m 25s (- 1m 11s) (72500 96%) 0.5645\n",
      "34m 28s (- 1m 8s) (72600 96%) 0.6093\n",
      "34m 31s (- 1m 5s) (72700 96%) 0.6718\n",
      "34m 34s (- 1m 2s) (72800 97%) 0.7013\n",
      "34m 37s (- 0m 59s) (72900 97%) 0.6587\n",
      "34m 40s (- 0m 56s) (73000 97%) 0.5606\n",
      "34m 42s (- 0m 54s) (73100 97%) 0.6118\n",
      "34m 45s (- 0m 51s) (73200 97%) 0.4697\n",
      "34m 48s (- 0m 48s) (73300 97%) 0.6570\n",
      "34m 51s (- 0m 45s) (73400 97%) 0.6859\n",
      "34m 54s (- 0m 42s) (73500 98%) 0.5601\n",
      "34m 56s (- 0m 39s) (73600 98%) 0.5252\n",
      "34m 59s (- 0m 37s) (73700 98%) 0.6103\n",
      "35m 2s (- 0m 34s) (73800 98%) 0.5903\n",
      "35m 5s (- 0m 31s) (73900 98%) 0.6625\n",
      "35m 8s (- 0m 28s) (74000 98%) 0.5664\n",
      "35m 11s (- 0m 25s) (74100 98%) 0.5932\n",
      "35m 14s (- 0m 22s) (74200 98%) 0.5350\n",
      "35m 17s (- 0m 19s) (74300 99%) 0.5837\n",
      "35m 20s (- 0m 17s) (74400 99%) 0.5514\n",
      "35m 23s (- 0m 14s) (74500 99%) 0.6178\n",
      "35m 26s (- 0m 11s) (74600 99%) 0.4973\n",
      "35m 29s (- 0m 8s) (74700 99%) 0.5629\n",
      "35m 31s (- 0m 5s) (74800 99%) 0.5832\n",
      "35m 34s (- 0m 2s) (74900 99%) 0.6589\n",
      "35m 37s (- 0m 0s) (75000 100%) 0.5736\n"
     ]
    }
   ],
   "source": [
    "hidden_size = 256\n",
    "encoder1 = EncoderRNN(input_lang.n_words, hidden_size).to(device)\n",
    "decoder1 = AttentionDecoderRNN(hidden_size, output_lang.n_words, dropout_p=0.1).to(device)\n",
    "\n",
    "train_iterations(encoder1, decoder1, 75000, print_every=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wbakD4a1-MF3"
   },
   "source": [
    "Loss: 0.5736"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pG13VLVH-U4h"
   },
   "outputs": [],
   "source": [
    "def evaluate(encoder, decoder, sentence, max_length=MAX_LENGTH):\n",
    "    with torch.no_grad():\n",
    "        input_tensor = sentence_to_tensor(input_lang, sentence)\n",
    "        input_length = input_tensor.size()[0]\n",
    "        encoder_hidden = encoder.init_hidden_state()\n",
    "\n",
    "        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "\n",
    "        # Unroll Encoder RNN\n",
    "        for t in range(input_length):\n",
    "            encoder_output, encoder_hidden = encoder(input_tensor[t], encoder_hidden)\n",
    "            encoder_outputs[t] += encoder_output[0, 0]\n",
    "\n",
    "        # Initialize Decoder Hidden State as the last Encoder Hidden State\n",
    "        decoder_input = torch.tensor([[SOS_token]], device=device)  # SOS\n",
    "        decoder_hidden = encoder_hidden\n",
    "\n",
    "        decoded_words = []\n",
    "        decoder_attentions = torch.zeros(max_length, max_length)\n",
    "\n",
    "        # Unroll Decoder until most probable tokens is EOS\n",
    "        for dt in range(max_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
    "            decoder_attentions[dt] = decoder_attention.data\n",
    "\n",
    "            # Get index of most probable token (argmax)\n",
    "            _, argmax = decoder_output.data.topk(1)\n",
    "            \n",
    "            # Stop unrolling if EOS token is most probable\n",
    "            if argmax.item() == EOS_token:\n",
    "                decoded_words.append('<EOS>')\n",
    "                break\n",
    "            # If not EOS token, append the most probable word (convert index to string)\n",
    "            else:\n",
    "                decoded_words.append(output_lang.index2word[argmax.item()])\n",
    "            \n",
    "            # Stop keeping track of gradients..? i.e detach() function\n",
    "            decoder_input = argmax.squeeze().detach()\n",
    "\n",
    "        return decoded_words, decoder_attentions[:dt + 1]\n",
    "\n",
    "def evaluate_randomly(encoder, decoder, n=10):\n",
    "    for i in range(n):\n",
    "        source, target = random.choice(pairs)\n",
    "        print('SOURCE:', source)\n",
    "        print('TARGET:', target)\n",
    "        print('...')\n",
    "        # Translate source sentence\n",
    "        output_words, attentions = evaluate(encoder, decoder, source)\n",
    "        output_sentence = ' '.join(output_words)\n",
    "        print('PREDICTED:', output_sentence)\n",
    "        print('-'*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 905
    },
    "colab_type": "code",
    "id": "vab5Xx7akOWT",
    "outputId": "290d54fd-bb6f-49cc-aa10-aff80ce0f8e3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SOURCE: je suis plutot occupe .\n",
      "TARGET: i m rather busy .\n",
      "...\n",
      "PREDICTED: i m pretty busy . <EOS>\n",
      "--------------------------------------------------\n",
      "SOURCE: on est enfin seuls .\n",
      "TARGET: we re finally alone .\n",
      "...\n",
      "PREDICTED: we re finally alone . <EOS>\n",
      "--------------------------------------------------\n",
      "SOURCE: nous sommes tres occupees .\n",
      "TARGET: we re very busy .\n",
      "...\n",
      "PREDICTED: we re very busy . <EOS>\n",
      "--------------------------------------------------\n",
      "SOURCE: nous sommes en charge .\n",
      "TARGET: we re in charge .\n",
      "...\n",
      "PREDICTED: we re in charge . <EOS>\n",
      "--------------------------------------------------\n",
      "SOURCE: quelle commere tu fais .\n",
      "TARGET: you re such a tattletale .\n",
      "...\n",
      "PREDICTED: you are such the one . . <EOS>\n",
      "--------------------------------------------------\n",
      "SOURCE: je suis souvent en difficulte .\n",
      "TARGET: i am often in difficulties .\n",
      "...\n",
      "PREDICTED: i m in in trouble . <EOS>\n",
      "--------------------------------------------------\n",
      "SOURCE: nous sommes vieux amis .\n",
      "TARGET: we re old friends .\n",
      "...\n",
      "PREDICTED: we re old friends . <EOS>\n",
      "--------------------------------------------------\n",
      "SOURCE: j apprends aussi le francais .\n",
      "TARGET: i m also learning french .\n",
      "...\n",
      "PREDICTED: i am also learning french french . <EOS>\n",
      "--------------------------------------------------\n",
      "SOURCE: il souffre d amnesie .\n",
      "TARGET: he is suffering from loss of memory .\n",
      "...\n",
      "PREDICTED: he is suffering from from of . . <EOS>\n",
      "--------------------------------------------------\n",
      "SOURCE: nous sommes supposes connaitre les regles .\n",
      "TARGET: we are supposed to know the rules .\n",
      "...\n",
      "PREDICTED: we are supposed to know know rules . <EOS>\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "evaluate_randomly(encoder1, decoder1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load trained model instead\n",
    "encoder_dict = torch.load('saved-models/encoder-rnn.model', map_location=torch.device(device))\n",
    "decoder_dict = torch.load('saved-models/decoder-rnn-att.model', map_location=torch.device(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 256\n",
    "encoder1 = EncoderRNN(input_lang.n_words, hidden_size).to(device)\n",
    "decoder1 = AttentionDecoderRNN(hidden_size, output_lang.n_words, dropout_p=0.1).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder1.load_state_dict(encoder_dict)\n",
    "decoder1.load_state_dict(decoder_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SOURCE: ce n est qu une enfant .\n",
      "TARGET: she s just a child .\n",
      "...\n",
      "PREDICTED: she s just a child . <EOS>\n",
      "--------------------------------------------------\n",
      "SOURCE: elles ont de la chance d etre vivantes .\n",
      "TARGET: they re lucky to be alive .\n",
      "...\n",
      "PREDICTED: they re lucky to be alive . <EOS>\n",
      "--------------------------------------------------\n",
      "SOURCE: je suis fier de vous les gars .\n",
      "TARGET: i m proud of you guys .\n",
      "...\n",
      "PREDICTED: i m proud of you guys . <EOS>\n",
      "--------------------------------------------------\n",
      "SOURCE: ils sont dans la douche .\n",
      "TARGET: they re in the shower .\n",
      "...\n",
      "PREDICTED: they re in the shower . <EOS>\n",
      "--------------------------------------------------\n",
      "SOURCE: il est japonais de naissance .\n",
      "TARGET: he is japanese by birth .\n",
      "...\n",
      "PREDICTED: he is japanese in japanese . <EOS>\n",
      "--------------------------------------------------\n",
      "SOURCE: je ne suis pas libre cette apres midi .\n",
      "TARGET: i m not free to go this afternoon .\n",
      "...\n",
      "PREDICTED: i m not free to go free afternoon afternoon .\n",
      "--------------------------------------------------\n",
      "SOURCE: encore un petit effort .\n",
      "TARGET: we re almost there .\n",
      "...\n",
      "PREDICTED: we re almost there . <EOS>\n",
      "--------------------------------------------------\n",
      "SOURCE: elle est trop vieille pour toi .\n",
      "TARGET: she s too old for you .\n",
      "...\n",
      "PREDICTED: she s too old for you . <EOS>\n",
      "--------------------------------------------------\n",
      "SOURCE: nous sommes tous contents d aider .\n",
      "TARGET: we are all happy to help .\n",
      "...\n",
      "PREDICTED: we re all happy to help . <EOS>\n",
      "--------------------------------------------------\n",
      "SOURCE: tu rougis !\n",
      "TARGET: you re blushing .\n",
      "...\n",
      "PREDICTED: you re blushing . <EOS>\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "evaluate_randomly(encoder1, decoder1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "translation-rnn.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
